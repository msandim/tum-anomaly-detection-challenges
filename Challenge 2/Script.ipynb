{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Challenge 1\n",
    "## Miguel Sandim and Paula Fortuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Solve format problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 -  yelp_data_train.dat and yelp_data_test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a text editor (e. g. sublime) use regex.\n",
    "\n",
    "1) Replace \" by \"\"\n",
    "\n",
    "2) Surround text field with \"\n",
    "To match the first one use this (dont forget to remove the one that appears also in the begining of the sentence, and the one in the header):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "^[^;]*;[^;]*;[^;]*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to find the last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ";[^;]*;[^;]*;[^;]*;[^;]*;[^;]*;[^;]*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) check if all the lines match the refered structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "^[^;]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Remove random newlines that appear in the rows and make new instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 - yelp_data_reviewer.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) ; caracter removed from fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) smiles removed from fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 hotel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) removal of ; in the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load data (finally!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Random libraries and seeds:\n",
    "import random\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "# read from csv\n",
    "\n",
    "train_df = pd.read_csv(\"data/yelp_data_train.dat\", sep = ';', encoding = 'utf-8')\n",
    "test_df = pd.read_csv(\"data/yelp_data_test.dat\", sep = ';', encoding = 'utf-8')\n",
    "reviewers_df = pd.read_csv(\"data/yelp_data_reviewer.dat\", sep = ';', encoding = 'utf-8')\n",
    "hotels_df = pd.read_csv(\"data/yelp_data_hotel.dat\", sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.iloc[:,36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "479/(3956 + 479)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About 10% of our dataset are anomalous cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = train_df[len(train_df.columns)-1]\n",
    "X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "num_rows_X_train = X_train[0].count()\n",
    "num_columns_X_train = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "######################################\n",
    "# Function Save Data To CSV\n",
    "######################################\n",
    "\n",
    "def saveDataToCSV(Y_pred):\n",
    "    id_list = range(1, len(Y_pred)+1)\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": id_list,\n",
    "        \"Expected\": Y_pred\n",
    "    })\n",
    "    submission = submission[['Id', 'Expected']]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy.ma as ma\n",
    "import math\n",
    "\n",
    "######################################\n",
    "# Strategies based on constants\n",
    "######################################\n",
    "\n",
    "#replace by zero - black colour in RGB\n",
    "def replaceByZero(df):\n",
    "    return np.nan_to_num(df)\n",
    "\n",
    "#replace by 255 - white colour in RGB\n",
    "def replaceBy255(df):\n",
    "    return df.fillna(255)\n",
    "\n",
    "#####################################\n",
    "# Strategies based on columns values\n",
    "#####################################\n",
    "\n",
    "#column minimum\n",
    "def replaceByColumnMinimum(df):\n",
    "    return df.fillna(df.min())\n",
    "\n",
    "#column maximum\n",
    "def replaceByColumnMaximum(df):\n",
    "    return df.fillna(df.max())\n",
    "\n",
    "#column mean\n",
    "def replaceByColumnMean(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "#column median\n",
    "def replaceByColumnMedian(df):\n",
    "    return df.fillna(df.median())\n",
    "\n",
    "#####################################\n",
    "# Strategies based on rows values\n",
    "#####################################\n",
    "\n",
    "# in the analysis of the rows we have to take into account that each four consecutive rows describe a pixel.\n",
    "# Each of these four rows stands for: Red, Green, IR, IR. |R|G|IR1|IR2|\n",
    "# These values refer to different things, and therefore are analysed independently.\n",
    "# this will allow to consider more the specificities of the problem\n",
    "\n",
    "# divide the data into four datasets, corresponding to each type of values \n",
    "\n",
    "#general function to gather columns for each type |R|G|IR1|IR2| using the mod operator\n",
    "def separatePixelColumns(position, df):\n",
    "    indexes = range(0, num_columns_X_train-1)\n",
    "    indexes = [x for x in indexes if x % 4 == position]\n",
    "    df_p_attribute = df.iloc[:,indexes]\n",
    "    return df_p_attribute\n",
    "\n",
    "#general function to fill missing values based on the rows\n",
    "#Note that does not make sense to consider the four values |R|G|IR1|IR2|, because they refer to different properties\n",
    "def fillMissingValuesByRow(df, function):\n",
    "    for index, row in df.iterrows():\n",
    "        value_without_nan = function(row)\n",
    "        nan_positions = row.isnull()\n",
    "        row[nan_positions] = value_without_nan\n",
    "    return df\n",
    "\n",
    "#row spectral mean\n",
    "def replaceByRowMean(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmean)\n",
    "\n",
    "#row spectral median\n",
    "def replaceByRowMedian(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmedian)\n",
    "\n",
    "#row spectral minimum\n",
    "def replaceByRowMinimum(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmin)\n",
    "\n",
    "#row spectral maximum\n",
    "def replaceByRowMaximum(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmax)\n",
    "\n",
    "########################################\n",
    "# Strategies based on data distribution\n",
    "########################################\n",
    "\n",
    "#consider the distribution of the spectral values of each type. \n",
    "#Get random value from the spectral values of same type\n",
    "\n",
    "    \n",
    "def getRandomNumberFromDataframe(df):\n",
    "    while True:\n",
    "        row = df.sample(1, random_state = 2)\n",
    "        values = row.values[0]\n",
    "        value = random.choice(values)\n",
    "        if not math.isnan(value):\n",
    "            break\n",
    "    return value\n",
    "        \n",
    "def fillMissingValuesWithDistribution(df):\n",
    "    for index, row in df.iterrows():\n",
    "        nan_positions = row.isnull()\n",
    "        for i in range(len(nan_positions)): \n",
    "            if nan_positions.iloc[i] == True:\n",
    "                value = getRandomNumberFromDataframe(df)\n",
    "                row.iloc[i] = value\n",
    "    return df\n",
    "\n",
    "#########################################%%%\n",
    "# fill Missing Values Considering Spectral\n",
    "#########################################%%%\n",
    "\n",
    "def fillMissingValuesBySpectral(df, function):\n",
    "\n",
    "    #generate four new datasets with the columns of each type\n",
    "    df_p_attribute_R = separatePixelColumns(0, df)\n",
    "    df_p_attribute_G = separatePixelColumns(1, df)\n",
    "    df_p_attribute_IR1 = separatePixelColumns(2, df)\n",
    "    df_p_attribute_IR2 = separatePixelColumns(3, df)\n",
    "\n",
    "    #apply function to each of the 4 datasets\n",
    "    df_p_attribute_R = function(df_p_attribute_R)\n",
    "    df_p_attribute_G = function(df_p_attribute_G)\n",
    "    df_p_attribute_IR1 = function(df_p_attribute_IR1)\n",
    "    df_p_attribute_IR2 = function(df_p_attribute_IR2)\n",
    "\n",
    "    df = pd.concat(\n",
    "                        [df_p_attribute_R, \n",
    "                         df_p_attribute_G,\n",
    "                         df_p_attribute_IR1,\n",
    "                         df_p_attribute_IR2], \n",
    "                        axis=1\n",
    "                        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#   CHAMADAS\n",
    "#######################\n",
    "\n",
    "#X_train = replaceByZero(X_train)\n",
    "#test_df = replaceByZero(test_df)\n",
    "\n",
    "#X_train = replaceBy255(X_train)\n",
    "#test_df = replaceBy255(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMinimum(X_train)\n",
    "#test_df = replaceByColumnMinimum(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMaximum(X_train)\n",
    "#test_df = replaceByColumnMaximum(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMean(X_train)\n",
    "#test_df = replaceByColumnMean(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMedian(X_train)\n",
    "#test_df = replaceByColumnMedian(test_df)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMean)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMean)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMedian)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMedian)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMinimum)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMinimum)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMaximum)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMaximum)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, fillMissingValuesWithDistribution)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, fillMissingValuesWithDistribution)\n",
    "\n",
    "datasets = {f_name: {\"train\": f(X_train.copy()), \"test\": f(test_df.copy())} for f_name, f in [\n",
    "        (\"01-zero\", replaceByZero),\n",
    "        (\"02-255\", replaceBy255),\n",
    "        (\"03-col-min\", replaceByColumnMinimum),\n",
    "        (\"04-col-max\", replaceByColumnMaximum),\n",
    "        (\"05-col-mean\", replaceByColumnMean),\n",
    "        (\"06-col-median\", replaceByColumnMedian),\n",
    "        (\"07-spec-mean\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMean)),\n",
    "        (\"08-spec-median\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMedian)),\n",
    "        (\"09-spec-min\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMinimum)),\n",
    "        (\"10-spec-max\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMaximum)),\n",
    "        (\"11-spec-dis\", lambda data: fillMissingValuesBySpectral(data, fillMissingValuesWithDistribution))\n",
    "    ]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Normalization\n",
    "\n",
    "There is no need for normalization in this dataset, since all features are between 0 and 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model Selection\n",
    "\n",
    "- First goal: discover which type of analyses works better\n",
    "- Second Goal: tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Model selection based on which models do best in CV using default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inspired in http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "import sklearn.model_selection as mds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# prepare data\n",
    "\n",
    "#Y_train = train_df[:,-1]\n",
    "#X_train = train_df[:,:-1]\n",
    "\n",
    "#Y_train = train_df[len(train_df.columns)-1]\n",
    "#X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM-Linear', SVC(kernel=\"linear\")))\n",
    "models.append(('SVM-Poly', SVC(kernel=\"poly\")))\n",
    "models.append(('SVM-RBF', SVC(kernel=\"rbf\")))\n",
    "models.append(('NN', MLPClassifier(alpha=1))) \n",
    "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)))\n",
    "models.append(('AB', AdaBoostClassifier()))\n",
    "models.append(('XGB', xgb.XGBClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = {}\n",
    "scoring = 'roc_auc' # try with 'roc_auc', f1'\n",
    "\n",
    "kfold = mds.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
    "\n",
    "for NA_strategy in sorted(datasets.keys()):\n",
    "    \n",
    "    results_by_strategy = []\n",
    "    \n",
    "    for model_name, model in models:\n",
    "        cv_results = mds.cross_val_score(model, datasets[NA_strategy][\"train\"], Y_train, cv=kfold, scoring=scoring)\n",
    "        results_by_strategy.append({\"name\": model_name, \"cv_results\": cv_results, \"mean\": cv_results.mean(), \"std\": cv_results.std()})\n",
    "        #print(\"%s: %f (%f)\" % (model_name, cv_results.mean(), cv_results.std()))\n",
    "        \n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure(figsize=(13, 5), dpi=500)\n",
    "    fig.suptitle('Algorithm Comparison using \\\"%s\\\"' % NA_strategy)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot([x[\"cv_results\"] for x in results_by_strategy])\n",
    "    ax.set_xticklabels([x[\"name\"] for x in results_by_strategy])\n",
    "    plt.show()\n",
    "    \n",
    "    # order the models by the mean auc\n",
    "    results_by_strategy.sort(key=lambda x: x[\"mean\"], reverse=True)\n",
    "    print([(x[\"name\"], x[\"mean\"]) for x in results_by_strategy])\n",
    "    \n",
    "    results[NA_strategy] = results_by_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results sorted by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_sorted = [(na_method, algorithm[\"name\"], algorithm[\"mean\"]) for na_method in results for algorithm in results[na_method]]\n",
    "results_sorted.sort(key=lambda x: x[2], reverse=True)\n",
    "results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSupervisedAlgorithmsDefault(inf, sup):\n",
    "    plt.figure(figsize=(13, 7), dpi=500)\n",
    "    \n",
    "    # x axis\n",
    "    labels = [na_method for na_method in results]\n",
    "    labels.sort()\n",
    "    plt.xticks(np.arange(len(labels)), labels, rotation='vertical')\n",
    "    plt.ylim(inf, sup)\n",
    "    \n",
    "    # legend:\n",
    "    algorithm_names = [x[\"name\"] for x in results[\"01-zero\"]] \n",
    "    \n",
    "    [plt.plot([[x[\"mean\"] for x in results[na_method] if x[\"name\"] == alg_name] for na_method in sorted(results)],\n",
    "              label = alg_name) for alg_name in algorithm_names]\n",
    "    \n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('NA-filling method')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "plotSupervisedAlgorithmsDefault(0.69, 1)\n",
    "plotSupervisedAlgorithmsDefault(0.9, 1)\n",
    "plotSupervisedAlgorithmsDefault(0.99, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on this plot, we decided to tune XGB and LDA and use 07-spec-mean and 09-spec-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotConfusionMatrixBestXGB():\n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    train, test = kfold.split(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "\n",
    "    lol = xgb.XGBClassifier().fit(datasets[\"07-spec-mean\"][\"train\"].iloc[train[0]], Y_train[train[0]])\n",
    "\n",
    "    train1_pred = lol.predict(datasets[\"07-spec-mean\"][\"train\"].iloc[train[1]])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[train[1]], train1_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    \n",
    "plotConfusionMatrixBestXGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotConfusionMatrixBestLDA():\n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    train, test = kfold.split(datasets[\"09-spec-min\"][\"train\"], Y_train)\n",
    "\n",
    "    lol = xgb.XGBClassifier().fit(datasets[\"09-spec-min\"][\"train\"].iloc[train[0]], Y_train[train[0]])\n",
    "\n",
    "    train1_pred = lol.predict(datasets[\"09-spec-min\"][\"train\"].iloc[train[1]])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[train[1]], train1_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    \n",
    "plotConfusionMatrixBestLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Tuning of the best models\n",
    "#### Based on this plot, we decided to tune LDA and XGB\n",
    "### Tuning XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "def modelfit(alg, train_predictors, train_target, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_predictors.values, label=train_target.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_predictors, train_target, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(train_predictors)\n",
    "    dtrain_predprob = alg.predict_proba(train_predictors)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_target.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_target, dtrain_predprob))\n",
    "                    \n",
    "    #feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    #feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    #plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "def tuneXGB1():\n",
    "    xgb1 = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5, # This should be between 3-10\n",
    "        min_child_weight=1, # A smaller value is chosen because it is a highly imbalanced class problem\n",
    "        gamma=0,\n",
    "        subsample=0.8, # Typical values range between 0.5-0.9.\n",
    "        colsample_bytree=0.8, # Typical values range between 0.5-0.9.\n",
    "        objective= 'binary:logistic',\n",
    "        #nthread=4,\n",
    "        scale_pos_weight=1, # Because of high class imbalance\n",
    "        seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This last result seems too good to be truth?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB2():\n",
    "    param_test1 = {\n",
    "        'max_depth': np.arange(3,10,2),\n",
    "        'min_child_weight': np.arange(1,6,2)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=5,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### {'max_depth': 7, 'min_child_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB3():\n",
    "    param_test1 = {\n",
    "        'max_depth': [6,7,8],\n",
    "        'min_child_weight': [1,2,3]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=5,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB4():\n",
    "    param_test1 = {\n",
    "        'gamma':[i/10.0 for i in np.arange(0,5)]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gamma = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB5():\n",
    "    xgb1 = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=7, # This should be between 3-10\n",
    "        min_child_weight=1, # A smaller value is chosen because it is a highly imbalanced class problem\n",
    "        gamma=0,\n",
    "        subsample=0.8, # Typical values range between 0.5-0.9.\n",
    "        colsample_bytree=0.8, # Typical values range between 0.5-0.9.\n",
    "        objective= 'binary:logistic',\n",
    "        #nthread=4,\n",
    "        scale_pos_weight=1, # Because of high class imbalance\n",
    "        seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB6():\n",
    "    param_test1 = {\n",
    "     'subsample': np.arange(0.6, 1.0, 0.1),\n",
    "     'colsample_bytree': np.arange(0.6, 1.0, 0.1)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample = 0.6 and colsample_bytree = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB7():\n",
    "    param_test1 = {\n",
    "     'subsample': np.arange(0.55, 0.7, 0.05),\n",
    "     'colsample_bytree': np.arange(0.85, 1.0, 0.05)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### better tuned: subsample=0.55 and colsample_bytree=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB8():\n",
    "    param_test1 = {\n",
    "     'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reg alpha = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tuneXGB9():    \n",
    "    xgb1 = xgb.XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=7,\n",
    "                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "                      reg_alpha=1e-5, objective= 'binary:logistic', scale_pos_weight=1, seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneLDA():\n",
    "    param_test1 = [{\"solver\": [\"svd\"], \"n_components\": np.arange(1,len(X_train.columns) - 1)},\n",
    "                   {\"solver\": [\"lsqr\", \"eigen\"], \"n_components\": np.arange(1,len(X_train.columns) - 1), \"shrinkage\": [\"auto\"]}]\n",
    "        \n",
    "    gsearch1 = GridSearchCV(estimator=LinearDiscriminantAnalysis(), \n",
    "                            param_grid = param_test1, scoring='roc_auc', cv=10)\n",
    "    \n",
    "    fit = gsearch1.fit(datasets[\"09-spec-min\"][\"train\"], Y_train)\n",
    "    return(fit)\n",
    "    \n",
    "bestLDAfit = tuneLDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestLDAfit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestLDAfit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluatingBestLDA():\n",
    "    kfold = mds.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
    "    model = LinearDiscriminantAnalysis(n_components = 1, shrinkage = \"auto\", solver=\"lsqr\")\n",
    "    \n",
    "    cv_results = mds.cross_val_score(model, datasets[\"09-spec-min\"][\"train\"], Y_train, cv=kfold, scoring=\"roc_auc\")\n",
    "    print(cv_results.mean())\n",
    "    \n",
    "evaluatingBestLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Unsupervised Anomaly Detection Methods\n",
    "We decided to try LOF and see how it goes. We used our implementation from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lof_pal as lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makePredictonsLOF():\n",
    "    outliers = []\n",
    "    \n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    sets,_ = kfold.split(datasets[\"07-spec-mean\"][\"train\"], Y_train)                                      \n",
    "    \n",
    "    # Train with only positive examples:\n",
    "    l = lof.LOF(datasets[\"07-spec-mean\"][\"train\"].iloc[sets[0]][Y_train[sets[0]] != 1], 3)\n",
    "    \n",
    "    Y_pred = [1 if x > 1.2 else 0 for x in l.predict(datasets[\"07-spec-mean\"][\"train\"].iloc[sets[1]])]\n",
    "        \n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[sets[1]], Y_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    #return Y_pred\n",
    "        \n",
    "    \n",
    "makePredictonsLOF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose algorithm\n",
    "def makeSubmissionKaggle(NA_strategy, algorithm):\n",
    "    print(\"Submiting using \\\"%s\\\"\" % (NA_strategy))\n",
    "\n",
    "    algorithm.fit(datasets[NA_strategy][\"train\"], Y_train)\n",
    "    Y_pred = algorithm.predict(datasets[NA_strategy][\"test\"])\n",
    "    Y_pred = Y_pred.astype(int)\n",
    "\n",
    "    # save data to CSV\n",
    "    saveDataToCSV(Y_pred)\n",
    "    \n",
    "#makeSubmissionKaggle(\"07-spec-mean\", xgb.XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=7,\n",
    "#                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "#                      reg_alpha=1e-5, objective= 'binary:logistic', scale_pos_weight=1, seed=2))\n",
    "\n",
    "#makeSubmissionKaggle(\"09-spec-min\", LinearDiscriminantAnalysis(n_components = 1, shrinkage = \"auto\", solver=\"lsqr\"))\n",
    "makeSubmissionKaggle(\"10-spec-max\", AdaBoostClassifier())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
