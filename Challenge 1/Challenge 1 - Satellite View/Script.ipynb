{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#read from csv\n",
    "test_df = pd.read_csv(\"sat-test-data.csv.dat\", header=None)\n",
    "train_df = pd.read_csv(\"sat-train.csv.dat\", header =None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df\n",
    "train_df\n",
    "Y_train = train_df[len(train_df.columns)-1]\n",
    "X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "num_rows_X_train = X_train[0].count()\n",
    "num_columns_X_train = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     4     8     12    16    20    24    28    32\n",
      "0  80.0  76.0  76.0  76.0  76.0   NaN  79.0  79.0   NaN\n",
      "1  76.0  76.0  76.0   NaN  80.0  80.0  79.0   NaN  79.0\n",
      "2  80.0  76.0   NaN  80.0  80.0  80.0  79.0  79.0   NaN\n",
      "3  76.0  76.0  76.0  80.0  80.0  80.0  79.0  79.0  79.0\n",
      "4  76.0  76.0  76.0  80.0  80.0  80.0  79.0  79.0  75.0\n",
      "     0     4     8     12    16    20    24    28    32\n",
      "0  80.0  76.0  76.0  76.0  76.0  78.0  79.0  79.0  63.0\n",
      "1  76.0  76.0  76.0  70.0  80.0  80.0  79.0  56.0  79.0\n",
      "2  80.0  76.0  52.0  80.0  80.0  80.0  79.0  79.0  74.0\n",
      "3  76.0  76.0  76.0  80.0  80.0  80.0  79.0  79.0  79.0\n",
      "4  76.0  76.0  76.0  80.0  80.0  80.0  79.0  79.0  75.0\n"
     ]
    }
   ],
   "source": [
    "import  scipy.stats as stats\n",
    "import numpy.ma as ma\n",
    "import math\n",
    "\n",
    "######replace by zero - black colour in RGB\n",
    "#train_df = np.nan_to_num(train_df)\n",
    "#test_df = np.nan_to_num(test_df)\n",
    "\n",
    "######replace by 255 - white colour in RGB\n",
    "#train_df = train_df.fillna(255)\n",
    "#test_df = train_df.fillna(255)\n",
    "\n",
    "######column minimum\n",
    "#train_df = train_df.fillna(train_df.min())\n",
    "#test_df = test_df.fillna(test_df.min())\n",
    "\n",
    "######column maximum\n",
    "#train_df = train_df.fillna(train_df.max())\n",
    "#test_df = test_df.fillna(test_df.max())\n",
    "\n",
    "\n",
    "#column mean\n",
    "#train_df = train_df.fillna(train_df.mean())\n",
    "#test_df = test_df.fillna(test_df.mean())\n",
    "\n",
    "#column median \n",
    "#train_df = train_df.fillna(train_df.median())\n",
    "#test_df = test_df.fillna(test_df.median())\n",
    "\n",
    "# in the analysis of the rows we have to take into account that each four consecutive rows describe a pixel.\n",
    "# Each of these four rows stands for: Green, R, IR, IR. These values refer to different things, and are analysed independently because of that.\n",
    "\n",
    "\n",
    "\n",
    "#make four datasets. dataframe is a group of 9 pixels times 4 values for each pixel: |R|G|IR1|IR2\n",
    "#this will allow to consider more the specificities of the problem\n",
    "\n",
    "#make list with the indexes of all R\n",
    "def separatePixelColumns( position ):\n",
    "    indexes = range(0, num_columns_X_train-1)\n",
    "    indexes = [x for x in indexes if x % 4 == position]\n",
    "    df_p_attribute = test_df.iloc[:,indexes]\n",
    "    return df_p_attribute\n",
    "\n",
    "df_p_attribute_R = separatePixelColumns(0)\n",
    "df_p_attribute_G = separatePixelColumns(1)\n",
    "df_p_attribute_IR1 = separatePixelColumns(2)\n",
    "df_p_attribute_IR2 = separatePixelColumns(3)\n",
    "\n",
    "def fillMissingValuesByRow(df, function):\n",
    "    for index, row in df.iterrows():\n",
    "        value_without_nan = function(row)\n",
    "        nan_positions = row.isnull()\n",
    "        row[nan_positions] = value_without_nan\n",
    "    return df\n",
    "\n",
    "#for index, row in df_p_attribute_R.iterrows():\n",
    "#        value_without_nan = np.nanmean(row)\n",
    "#        nan_positions = row.isnull()\n",
    "#        row[nan_positions] = value_without_nan\n",
    "\n",
    "#row spectral mean\n",
    "\n",
    "#df_p_attribute_R = fillMissingValues(df_p_attribute_R, np.nanmean)\n",
    "\n",
    "#row spectral median\n",
    "\n",
    "#df_p_attribute_R = fillMissingValues(df_p_attribute_R, np.nanmedian)\n",
    "#row spectral minimum\n",
    "\n",
    "#df_p_attribute_R = fillMissingValues(df_p_attribute_R, np.nanmin)\n",
    "#row spectral maximum\n",
    "\n",
    "#df_p_attribute_R = fillMissingValues(df_p_attribute_R, np.nanmax)\n",
    "\n",
    "# distribution of the spectral values. Get random value from this sample\n",
    "\n",
    "    \n",
    "def getRandomNumberFromDataframe(df):\n",
    "    import random\n",
    "\n",
    "    while True:\n",
    "        row = df_p_attribute_R.sample(1)\n",
    "        values = row.values[0]\n",
    "        value = random.choice(values)\n",
    "        if not math.isnan(value):\n",
    "            break\n",
    "    return value\n",
    "        \n",
    "def fillMissingValuesWithDistribution(df):\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "print(df_p_attribute_R.head())\n",
    "\n",
    "for index, row in df_p_attribute_R.iterrows():\n",
    "    nan_positions = row.isnull()\n",
    "    for i in range(len(nan_positions)): \n",
    "        if nan_positions.iloc[i] == True:\n",
    "            value = getRandomNumberFromDataframe(df_p_attribute_R)\n",
    "            row.iloc[i] = value\n",
    "            \n",
    "\n",
    "print(df_p_attribute_R.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Normalization\n",
    "\n",
    "There is no need of normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Choose Model\n",
    "\n",
    "Nota: adicionar neural networks - new in v 0.18\n",
    "\n",
    "First goal: discover which type of analyses works better\n",
    "Second Goal: tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'slice'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-659b304bc7fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[1;31m# prepare data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2055\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2056\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2057\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2064\u001b[0;31m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[1;34m\"\"\"Return the cached item, item represents a label indexer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_item_cache\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'slice'"
     ]
    }
   ],
   "source": [
    "#inspired in http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# prepare data\n",
    "\n",
    "Y_train = train_df[:,-1]\n",
    "X_train = train_df[:,:-1]\n",
    "\n",
    "#Y_train = train_df[len(train_df.columns)-1]\n",
    "#X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "seed = 7\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('NN', MLPClassifier(alpha=1))) \n",
    "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)))\n",
    "models.append(('AB', AdaBoostClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy' # try with 'roc_auc', f1'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "    cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission in Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ..., 1997 1998 1999]\n",
      "[0 0 0 ..., 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "adaboost = AdaBoostClassifier()\n",
    "\n",
    "adaboost.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = adaboost.predict(test_df)\n",
    "\n",
    "adaboost.score(X_train, Y_train)\n",
    "\n",
    "print(np.arange(Y_pred.size))\n",
    "\n",
    "Y_pred = Y_pred.astype(int)\n",
    "#np.hstack((Y_pred, np.vstack(np.arange(Y_pred.size))))\n",
    "\n",
    "print(Y_pred)\n",
    "np.savetxt(\"Pauliguel.csv\", Y_pred, delimiter=\",\", fmt='%u')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "- Poderíamos tentar fazer alguma coisa a ver com anomaly, porque até agora é uma análise muito geral:\n",
    "-- Pedir frequências das duas classes.\n",
    "-- Pedir um gráfico com as distribuições dos valores para cada espectro.\n",
    "-- Ver resultado quando se lida com os Missing values pela estratégia da distribuição\n",
    "-- ter o cuidado de ver se os resultados não estão a ser influenciados porque o nosso algoritmo diz sempre a mesma classe: ver precision e recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ReadData\n",
    "#XTrain, YTrain, rawTestData = importDataFromFiles()\n",
    "\n",
    "#handle missing values\n",
    "\n",
    "#for each different strategy for the missing values check which is the best model in the, get values for accuracy, auc, F1\n",
    "#check both on the K fold, and on the test dataset\n",
    "\n",
    "#pick the best strategy for missing values and  the best model and submit to kaggle \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
