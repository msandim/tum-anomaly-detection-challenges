{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Challenge 1\n",
    "## Miguel Sandim and Paula Fortuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Solve format problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a text editor (e. g. sublime) use regex and surround text with \"\n",
    "To match the first one use this (dont forget to remove the one that appears also in the begining of the sentence, and the one in the header):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "^[^;]*;[^;]*;[^;]*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to find the last:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ";[^;]*;[^;]*;[^;]*;[^;]*;[^;]*;[^;]*$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Random libraries and seeds:\n",
    "import random\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "# read from csv\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(\"data/yelp_data_train.dat\", sep = ';', encoding = 'utf-8')\n",
    "#test_df = pd.read_csv(\"data/yelp_data_test.dat\")\n",
    "#reviewers_df = pd.read_csv(\"data/yelp_data_reviewer.dat\")\n",
    "#hotels_df = pd.read_csv(\"data/yelp_data_hotel.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                       datereviewID              reviewerID  \\\n",
       "9/16/2010                       Ol  nf3q2h-kSQoZK2jBY92FOg   \n",
       "2/5/2010    i4HIAcNTjabdpG1K4F5Q2g  Sb3DJGdZ4Rq__CqxPbae-g   \n",
       "8/9/2010            veKKNAaSKWj8os  nR7zLyFOlzAYqmzgJ3DtXg   \n",
       "8/11/2012   6c-ZiQkHXtp1n6VfiKDQ3g  747lP4p8dUD6RTkcsIaSGg   \n",
       "7/9/2012    POWQ6FuUf3oe2ZkhmHvciA  Ij5t6VdwtasSkrpp9uAbKg   \n",
       "6/19/2012   QBynYcLgIgtAd-YfnrrAtA  hSERzClUe57bCw3nCp4plA   \n",
       "9/14/2012                   ELY3TK  OMm2VcGks3QL0p0n3_kPFw   \n",
       "3/20/2012   uWKWYb5vDpeDGEAZUc192g  yevHGEUQQmnVlBXIrJ885A   \n",
       "3/3/2012    hkt7Dnr7kRnLLd9pm-fxDw  Lql1_3zeGlny_Tgq4MI6Fg   \n",
       "6/18/2012   ZlexD7XvkqH8yve4zCAR7g  RtyDimVdIBwjGdQr0dti1w   \n",
       "3/13/2012   Rw1JmyRyyjoACCUFvmS9kQ  hCIJT7tIhPX_YZBCPhYhMg   \n",
       "5/24/2010                O9chyjQi5  nKgjmPhPPiJ8BL97dO76XA   \n",
       "10/25/2010  x4FHvju16JpVa3ihzIwQvw  IhKctrZ3BtJkfpf0qO-8mQ   \n",
       "2/13/2011                      NaN  yoB_PYQHjnPjh78ATA0Jgw   \n",
       "8/9/2011         9CPWfP7Ibj-2TthBN  b8B2b2q_Lcactxp8xr-jvg   \n",
       "9/21/2010   4sIRV-M-nrhyU5wHJSznrA  y5ptsWmvGEAftOQaiFhBcg   \n",
       "6/17/2009   s3V-CRnWtlHc2goC7xIAEA  QTJCwSaCfBz3T944SOiHBQ   \n",
       "4/4/2012    A5WJkZECjZVwSoV8Uxciww  _h5MGqTBM8J0tuAfbEvVDg   \n",
       "7/8/2011    xnZtuSIepywIkSC1uGYzGg  s6tSJ1NQMSU59lsCPYzzwQ   \n",
       "4/4/2011    FRM7-hER5S9wMf0IzciDcg  0oA4l1ob50aIZ9x9Kxobqg   \n",
       "8/23/2010   0fLzbPuBnOMKzVnmxqz-1w  3KM5g9hyaR-uE92WfMU6Cw   \n",
       "8/9/2008            MGcMncJSQQ7zzE  Nybw4RhZvdexOR-XgXD4DQ   \n",
       "4/24/2011   8a8MaeJ99mi3WTUZYDYL2Q  SKgjcQz0TNg8LKqlJwxjfg   \n",
       "10/3/2009   K8HSCX-GCOcF8VUsAtMSGQ  tdE3__i2otI_nL3M3sy0MQ   \n",
       "4/17/2011   2rAeHez1ZzyHvL1u2DIJsw  fOTzYtBF4lJTwDIMfCAeyQ   \n",
       "7/17/2010          07mEVsw7wUKuUOH  Uu-qEGsSb72ngIQUF85rDQ   \n",
       "10/26/2009                  Og0iVk  ptgFdw9WzPQx16w37XxsPQ   \n",
       "11/23/2011  JKcDTRG0dYs23kj1NeOB5A  zyk-YPhtFZK6kkbpzEKrWw   \n",
       "4/5/2012    6h8rBu1K3AxUiQE8biaTIg  T4LT4dPTTTVZocPhwrJboQ   \n",
       "3/24/2012   4uniW9GJpi56lRUyJFU-OQ  HfypTYNqexsCBuRBQFhKTA   \n",
       "...                            ...                     ...   \n",
       "5/1/2007    ELidp04R7jwXfXV5mFW1XQ  _f9NyNygDasxm4x_8K0FMg   \n",
       "7/14/2012   nJh1lZINVD5SAW2ecQir7A  aAnY0oKxg4WEcYsC9hw3cQ   \n",
       "3/5/2012                   QkDj-K2  YfsNifnBJYJ1aq05eC-u3g   \n",
       "9/14/2010   TFKtb8DG7yTAxkntYCRMTQ  uqC3ll6vCfqFalbWnPi-Qg   \n",
       "1/26/2010   QYD-28qlQRmlAe7G2FbNUA  mzO44cSLjgjo-TUYUqPyHA   \n",
       "8/3/2011    ZjhC8PHPq5RGocFrXTYcWw  oa027MtAk0PV7Tom9K8xHQ   \n",
       "5/27/2010   u5gKtqSW4os8H1OImGEkEw  b_54V-mRPPHIwj0wFWby-g   \n",
       "2/3/2009    VlJdOpIVNevAbBeESTofeQ  WauJzu-aZSJZuCLkLWFkag   \n",
       "10/4/2009   1bY3wvqJD3zOUx6b-HOb8g  OBz87AmKkY-RKP7dFQFugQ   \n",
       "9/4/2009    BMQbKdF_m2gGeY6hr9Fv1Q  Dv3r6dxp6N7B2S4stwS4dg   \n",
       "12/25/2011  M9jtbdwJYyxwWWufBaw3EA  -2aZoH_YaC-1s1BFzo7GcA   \n",
       "6/24/2011   LuMU5Me4cAMaxmoKWUrNZg  N0pY2Nd7xTZupPd3SYUWEA   \n",
       "11/30/2010  vE1oKqdlBCOfRDf7ObE_WQ  HWIJ4kkTMnuCTBkLmaupMA   \n",
       "6/28/2011   ux6ZfjP5yWHXU4zQA3P7yA  zJ2Op8FsslxF5TSem1f-zw   \n",
       "4/16/2010   HzOVUdfy4vAXsIm_eH-Pww  2ck_k8Swtj6-48kLR6IVWA   \n",
       "6/14/2011   D9urHoAJiktxTfpDy84vUw  MtR3yxthVe-LoHEAXUKSGw   \n",
       "6/4/2010    TYcOMlIhPaobaHFLywRLvw  2ynZkFzXxSYli0acUGEjxA   \n",
       "9/25/2011   ECKiMfnDzi11HdfeK0LIBw  pB_MluKOj16YTAH4ZozyNA   \n",
       "7/27/2012   W0KFuAk6Y62JTMdJw_7hWw  cHegKQwdtrdd3Hpo1b6NsA   \n",
       "5/18/2012   t9G4R6uGohktZeHZmf3kKA  nFSimJkp5uq8FPTyY-4NNQ   \n",
       "8/1/2011    f_MP8IYIvV_Sp3AgAFG0Iw  YMsf5cb9kbEj7DuPdyQ4eA   \n",
       "7/19/2011   6mqU5bF4Iq1ZHBfknBUsYQ  JwKrfYb2BNFRZdHCFlwO1w   \n",
       "6/16/2010   pXxaEx57oNIMqsLBqVczyg  hI8LGBnG-VupdJhsj_U3Og   \n",
       "4/6/2010    OTJHRZXJn8UfAJ4ljWN9sw  hU9lnr5ZAwWr0SHzg7RdHg   \n",
       "10/10/2008  IDSx8yGQKNUImss-4YUlGg  BaRtixU-lZsMr2PF4hrvqg   \n",
       "8/2/2012    lFkroXzrRhmMTrnn6__tnw  ZpFB_pqaUaDmIuCXbN87Ww   \n",
       "12/14/2011  mV2x61nyxzsKRgGbM3yuqg  twaozx1wvDOL0Z_fG9gclA   \n",
       "11/21/2009  CNwz6w426kCF5H6j-neQSg  S5L1xguLhXJWadpFXnRKqQ   \n",
       "11/20/2011  1eJPupHCsl-r3WUuz6QvTA  PJK4GsUItBU8JJuoAujWOA   \n",
       "8/5/2010    xwPMoEzuvpn3J32IvTcsiQ  MdYbNl_9Hm1CybsuC6UnkQ   \n",
       "\n",
       "                                                reviewContent  rating  \\\n",
       "9/16/2010   If you are considering staying here, watch thi...       1   \n",
       "2/5/2010    This place is disgusting, absolutely horrible,...       3   \n",
       "8/9/2010    Disgusting!!!  There is literally duct tape ho...       1   \n",
       "8/11/2012   This hotel came up on Hotwire for $108 a night...       4   \n",
       "7/9/2012    Good location, really run down. I am surprised...       2   \n",
       "6/19/2012   Beautiful lobby. The rest is a dump. The eleva...       1   \n",
       "9/14/2012   Stayed here when I went to Chicago for a weddi...       3   \n",
       "3/20/2012   I bleed SPG loyalty blood to the point where I...       1   \n",
       "3/3/2012    I stayed here a couple of times in 2011, as th...       3   \n",
       "6/18/2012   This is an older property, so the decor is dat...       1   \n",
       "3/13/2012   Small Rooms....one elevator that takes forever...       2   \n",
       "5/24/2010   Great location, terribly outdated. Feels like ...       2   \n",
       "10/25/2010  My husband and I came to Chicago for a week an...       4   \n",
       "2/13/2011   Stayed here over the Jan 15th weekend.  The lo...       4   \n",
       "8/9/2011    Breakfast no longer free, but the wifi is. The...       4   \n",
       "9/21/2010   I can't imagine paying full price to stay here...       2   \n",
       "6/17/2009   For the money.. it was a great hotel. Clean, g...       3   \n",
       "4/4/2012    Great location for walking to any part of down...       4   \n",
       "7/8/2011    Excellent location right down the street from ...       4   \n",
       "4/4/2011    Great location for a good price. Book it direc...       4   \n",
       "8/23/2010   Another Travelzoo deal at an awesome price of ...       2   \n",
       "8/9/2008    For the love of God, whatever you do, DO NOT t...       4   \n",
       "4/24/2011   Decent price for downtown Chicago over a weeke...       2   \n",
       "10/3/2009   Some rooms may reveal the building's age, but ...       4   \n",
       "4/17/2011   If you want a reasonably priced, clean, non-ch...       3   \n",
       "7/17/2010   I arrived at the Tremont this past weekend for...       2   \n",
       "10/26/2009  I have a fondness for older hotels that are a ...       4   \n",
       "11/23/2011  Well this was an interesting place.  I went to...       3   \n",
       "4/5/2012    The card key never worked.  I even got an extr...       1   \n",
       "3/24/2012   I won't go into details but basically employee...       1   \n",
       "...                                                       ...     ...   \n",
       "5/1/2007    Boo Marriott. You may have comfy beds but hosp...       1   \n",
       "7/14/2012   I attended a week-long training in their confe...       4   \n",
       "3/5/2012    I am torn with how I want to rate this hotel. ...       4   \n",
       "9/14/2010   Rooms are kinda old. Service is pretty mediocr...       3   \n",
       "1/26/2010   I agree with the first reviewer Celeste. I had...       2   \n",
       "8/3/2011    For the price I got this room for it was Great...       3   \n",
       "5/27/2010   I once found a key on the sidewalk near here, ...       1   \n",
       "2/3/2009    I was once told by the talent buyer at the Log...       1   \n",
       "10/4/2009   Where do I begin?? There are so many horrible ...       1   \n",
       "9/4/2009    This is obviously a place for junkies and hook...       5   \n",
       "12/25/2011  Over priced,  nickle and dime you for everythi...       1   \n",
       "6/24/2011   Housekeeping staff here needs lessons--dusty f...       2   \n",
       "11/30/2010  There are too many old men trying to pick up w...       2   \n",
       "6/28/2011   I used priceline to get a room at this hotel. ...       4   \n",
       "4/16/2010   When we got to the place where check in was ha...       1   \n",
       "6/14/2011   My boss booked two rooms here because the pric...       1   \n",
       "6/4/2010    We stayed in the motel because of it's conveni...       1   \n",
       "9/25/2011   Would recommend this hotel to anyone.  My wife...       5   \n",
       "7/27/2012   Fantastic location, friendly staff and nice ac...       4   \n",
       "5/18/2012   This boutique hotel is in an excellent locatio...       4   \n",
       "8/1/2011    My wife and I stayed here July 30th. We arrive...       2   \n",
       "7/19/2011   Very clean and super helpful staff.  The rooms...       4   \n",
       "6/16/2010   My company held an event here and the staff ov...       4   \n",
       "4/6/2010    This boutique hotel has a very personal approa...       5   \n",
       "10/10/2008  Wife and I stay here when we visit. Went up on...       5   \n",
       "8/2/2012    All Westins are definitely nice and so was thi...       4   \n",
       "12/14/2011  This place was awesome! Well except for the we...       4   \n",
       "11/21/2009  My friend and I, along with our mothers, staye...       5   \n",
       "11/20/2011  So Surprising when we arrived, The staff was g...       5   \n",
       "8/5/2010    Noise, noise, noise!  Unbelievable!  Between t...       1   \n",
       "\n",
       "            usefulCount  coolCount  funnyCount fake                 hotelID  \n",
       "9/16/2010             8          2           6    N  tQfLGoolUMu2J0igcWcoZg  \n",
       "2/5/2010             11          4           9    N  tQfLGoolUMu2J0igcWcoZg  \n",
       "8/9/2010              1          0           3    N  tQfLGoolUMu2J0igcWcoZg  \n",
       "8/11/2012             2          0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "7/9/2012              0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "6/19/2012             0          1           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "9/14/2012             2          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "3/20/2012             1          1           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "3/3/2012              0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "6/18/2012             0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "3/13/2012             0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "5/24/2010             2          1           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "10/25/2010            1          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "2/13/2011             0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "8/9/2011              1          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "9/21/2010             0          0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "6/17/2009             0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "4/4/2012              0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "7/8/2011              0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "4/4/2011              0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "8/23/2010             0          1           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "8/9/2008              0          0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "4/24/2011             0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "10/3/2009             0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "4/17/2011             0          0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "7/17/2010             2          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "10/26/2009            0          0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "11/23/2011            1          0           0    N  2nnXespKBBNtDQTtrumNFg  \n",
       "4/5/2012              1          0           0    N  2nnXespKBBNtDQTtrumNFg  \n",
       "3/24/2012             1          0           0    N  2nnXespKBBNtDQTtrumNFg  \n",
       "...                 ...        ...         ...  ...                     ...  \n",
       "5/1/2007              2          0           0    N  Gh4Tb2qCBUCOd5r7jqKnGA  \n",
       "7/14/2012             1          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "3/5/2012              3          1           1    N  BISUDalmPulSzHvsO3PhDA  \n",
       "9/14/2010             0          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "1/26/2010             2          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "8/3/2011              1          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "5/27/2010             4          2          11    N  bem-1CTpTNpArpCtThTmFw  \n",
       "2/3/2009              3          2           3    N  bem-1CTpTNpArpCtThTmFw  \n",
       "10/4/2009             0          0           0    Y  bem-1CTpTNpArpCtThTmFw  \n",
       "9/4/2009              0          0           0    Y  bem-1CTpTNpArpCtThTmFw  \n",
       "12/25/2011            0          0           0    Y  g51qDl6fQhgat-kFTrcbug  \n",
       "6/24/2011             0          0           0    Y  g51qDl6fQhgat-kFTrcbug  \n",
       "11/30/2010            0          0           0    Y  BISUDalmPulSzHvsO3PhDA  \n",
       "6/28/2011             0          0           0    Y  Gh4Tb2qCBUCOd5r7jqKnGA  \n",
       "4/16/2010             0          0           0    Y  Gh4Tb2qCBUCOd5r7jqKnGA  \n",
       "6/14/2011             0          0           0    Y  h-U_YfWBK2u_Vqo8AVA4KQ  \n",
       "6/4/2010              0          0           0    Y  h-U_YfWBK2u_Vqo8AVA4KQ  \n",
       "9/25/2011             0          0           0    Y  wLjR0DkA4zxu8iqbUQc0Og  \n",
       "7/27/2012             0          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "5/18/2012             0          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "8/1/2011              0          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "7/19/2011             0          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "6/16/2010             0          0           0    Y  S_qmb0Uzm_cNoRjSvZ6y1w  \n",
       "4/6/2010              0          0           0    Y  S_qmb0Uzm_cNoRjSvZ6y1w  \n",
       "10/10/2008            0          0           0    Y  S_qmb0Uzm_cNoRjSvZ6y1w  \n",
       "8/2/2012              0          0           0    Y  LUmAQaRrAleKdXZd8On16Q  \n",
       "12/14/2011            0          0           0    Y  PyG0aSX3pBx0hzoSH20FnA  \n",
       "11/21/2009            0          0           0    Y  PyG0aSX3pBx0hzoSH20FnA  \n",
       "11/20/2011            0          0           0    Y  gCdjyQeE0uRKCh7mVmnZzQ  \n",
       "8/5/2010              0          0           0    Y  rpP9iZsT3NC-Z4pUtQGoiA  \n",
       "\n",
       "[2908 rows x 9 columns]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.iloc[:,36].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "479/(3956 + 479)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About 10% of our dataset are anomalous cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_train = train_df[len(train_df.columns)-1]\n",
    "X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "num_rows_X_train = X_train[0].count()\n",
    "num_columns_X_train = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "######################################\n",
    "# Function Save Data To CSV\n",
    "######################################\n",
    "\n",
    "def saveDataToCSV(Y_pred):\n",
    "    id_list = range(1, len(Y_pred)+1)\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": id_list,\n",
    "        \"Expected\": Y_pred\n",
    "    })\n",
    "    submission = submission[['Id', 'Expected']]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy.ma as ma\n",
    "import math\n",
    "\n",
    "######################################\n",
    "# Strategies based on constants\n",
    "######################################\n",
    "\n",
    "#replace by zero - black colour in RGB\n",
    "def replaceByZero(df):\n",
    "    return np.nan_to_num(df)\n",
    "\n",
    "#replace by 255 - white colour in RGB\n",
    "def replaceBy255(df):\n",
    "    return df.fillna(255)\n",
    "\n",
    "#####################################\n",
    "# Strategies based on columns values\n",
    "#####################################\n",
    "\n",
    "#column minimum\n",
    "def replaceByColumnMinimum(df):\n",
    "    return df.fillna(df.min())\n",
    "\n",
    "#column maximum\n",
    "def replaceByColumnMaximum(df):\n",
    "    return df.fillna(df.max())\n",
    "\n",
    "#column mean\n",
    "def replaceByColumnMean(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "#column median\n",
    "def replaceByColumnMedian(df):\n",
    "    return df.fillna(df.median())\n",
    "\n",
    "#####################################\n",
    "# Strategies based on rows values\n",
    "#####################################\n",
    "\n",
    "# in the analysis of the rows we have to take into account that each four consecutive rows describe a pixel.\n",
    "# Each of these four rows stands for: Red, Green, IR, IR. |R|G|IR1|IR2|\n",
    "# These values refer to different things, and therefore are analysed independently.\n",
    "# this will allow to consider more the specificities of the problem\n",
    "\n",
    "# divide the data into four datasets, corresponding to each type of values \n",
    "\n",
    "#general function to gather columns for each type |R|G|IR1|IR2| using the mod operator\n",
    "def separatePixelColumns(position, df):\n",
    "    indexes = range(0, num_columns_X_train-1)\n",
    "    indexes = [x for x in indexes if x % 4 == position]\n",
    "    df_p_attribute = df.iloc[:,indexes]\n",
    "    return df_p_attribute\n",
    "\n",
    "#general function to fill missing values based on the rows\n",
    "#Note that does not make sense to consider the four values |R|G|IR1|IR2|, because they refer to different properties\n",
    "def fillMissingValuesByRow(df, function):\n",
    "    for index, row in df.iterrows():\n",
    "        value_without_nan = function(row)\n",
    "        nan_positions = row.isnull()\n",
    "        row[nan_positions] = value_without_nan\n",
    "    return df\n",
    "\n",
    "#row spectral mean\n",
    "def replaceByRowMean(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmean)\n",
    "\n",
    "#row spectral median\n",
    "def replaceByRowMedian(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmedian)\n",
    "\n",
    "#row spectral minimum\n",
    "def replaceByRowMinimum(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmin)\n",
    "\n",
    "#row spectral maximum\n",
    "def replaceByRowMaximum(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmax)\n",
    "\n",
    "########################################\n",
    "# Strategies based on data distribution\n",
    "########################################\n",
    "\n",
    "#consider the distribution of the spectral values of each type. \n",
    "#Get random value from the spectral values of same type\n",
    "\n",
    "    \n",
    "def getRandomNumberFromDataframe(df):\n",
    "    while True:\n",
    "        row = df.sample(1, random_state = 2)\n",
    "        values = row.values[0]\n",
    "        value = random.choice(values)\n",
    "        if not math.isnan(value):\n",
    "            break\n",
    "    return value\n",
    "        \n",
    "def fillMissingValuesWithDistribution(df):\n",
    "    for index, row in df.iterrows():\n",
    "        nan_positions = row.isnull()\n",
    "        for i in range(len(nan_positions)): \n",
    "            if nan_positions.iloc[i] == True:\n",
    "                value = getRandomNumberFromDataframe(df)\n",
    "                row.iloc[i] = value\n",
    "    return df\n",
    "\n",
    "#########################################%%%\n",
    "# fill Missing Values Considering Spectral\n",
    "#########################################%%%\n",
    "\n",
    "def fillMissingValuesBySpectral(df, function):\n",
    "\n",
    "    #generate four new datasets with the columns of each type\n",
    "    df_p_attribute_R = separatePixelColumns(0, df)\n",
    "    df_p_attribute_G = separatePixelColumns(1, df)\n",
    "    df_p_attribute_IR1 = separatePixelColumns(2, df)\n",
    "    df_p_attribute_IR2 = separatePixelColumns(3, df)\n",
    "\n",
    "    #apply function to each of the 4 datasets\n",
    "    df_p_attribute_R = function(df_p_attribute_R)\n",
    "    df_p_attribute_G = function(df_p_attribute_G)\n",
    "    df_p_attribute_IR1 = function(df_p_attribute_IR1)\n",
    "    df_p_attribute_IR2 = function(df_p_attribute_IR2)\n",
    "\n",
    "    df = pd.concat(\n",
    "                        [df_p_attribute_R, \n",
    "                         df_p_attribute_G,\n",
    "                         df_p_attribute_IR1,\n",
    "                         df_p_attribute_IR2], \n",
    "                        axis=1\n",
    "                        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#   CHAMADAS\n",
    "#######################\n",
    "\n",
    "#X_train = replaceByZero(X_train)\n",
    "#test_df = replaceByZero(test_df)\n",
    "\n",
    "#X_train = replaceBy255(X_train)\n",
    "#test_df = replaceBy255(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMinimum(X_train)\n",
    "#test_df = replaceByColumnMinimum(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMaximum(X_train)\n",
    "#test_df = replaceByColumnMaximum(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMean(X_train)\n",
    "#test_df = replaceByColumnMean(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMedian(X_train)\n",
    "#test_df = replaceByColumnMedian(test_df)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMean)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMean)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMedian)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMedian)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMinimum)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMinimum)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMaximum)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMaximum)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, fillMissingValuesWithDistribution)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, fillMissingValuesWithDistribution)\n",
    "\n",
    "datasets = {f_name: {\"train\": f(X_train.copy()), \"test\": f(test_df.copy())} for f_name, f in [\n",
    "        (\"01-zero\", replaceByZero),\n",
    "        (\"02-255\", replaceBy255),\n",
    "        (\"03-col-min\", replaceByColumnMinimum),\n",
    "        (\"04-col-max\", replaceByColumnMaximum),\n",
    "        (\"05-col-mean\", replaceByColumnMean),\n",
    "        (\"06-col-median\", replaceByColumnMedian),\n",
    "        (\"07-spec-mean\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMean)),\n",
    "        (\"08-spec-median\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMedian)),\n",
    "        (\"09-spec-min\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMinimum)),\n",
    "        (\"10-spec-max\", lambda data: fillMissingValuesBySpectral(data, replaceByRowMaximum)),\n",
    "        (\"11-spec-dis\", lambda data: fillMissingValuesBySpectral(data, fillMissingValuesWithDistribution))\n",
    "    ]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Normalization\n",
    "\n",
    "There is no need for normalization in this dataset, since all features are between 0 and 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model Selection\n",
    "\n",
    "- First goal: discover which type of analyses works better\n",
    "- Second Goal: tune the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Model selection based on which models do best in CV using default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#inspired in http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "import sklearn.model_selection as mds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# prepare data\n",
    "\n",
    "#Y_train = train_df[:,-1]\n",
    "#X_train = train_df[:,:-1]\n",
    "\n",
    "#Y_train = train_df[len(train_df.columns)-1]\n",
    "#X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "#models.append(('SVM-Linear', SVC(kernel=\"linear\")))\n",
    "models.append(('SVM-Poly', SVC(kernel=\"poly\")))\n",
    "models.append(('SVM-RBF', SVC(kernel=\"rbf\")))\n",
    "models.append(('NN', MLPClassifier(alpha=1))) \n",
    "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)))\n",
    "models.append(('AB', AdaBoostClassifier()))\n",
    "models.append(('XGB', xgb.XGBClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = {}\n",
    "scoring = 'roc_auc' # try with 'roc_auc', f1'\n",
    "\n",
    "kfold = mds.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
    "\n",
    "for NA_strategy in sorted(datasets.keys()):\n",
    "    \n",
    "    results_by_strategy = []\n",
    "    \n",
    "    for model_name, model in models:\n",
    "        cv_results = mds.cross_val_score(model, datasets[NA_strategy][\"train\"], Y_train, cv=kfold, scoring=scoring)\n",
    "        results_by_strategy.append({\"name\": model_name, \"cv_results\": cv_results, \"mean\": cv_results.mean(), \"std\": cv_results.std()})\n",
    "        #print(\"%s: %f (%f)\" % (model_name, cv_results.mean(), cv_results.std()))\n",
    "        \n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure(figsize=(13, 5), dpi=500)\n",
    "    fig.suptitle('Algorithm Comparison using \\\"%s\\\"' % NA_strategy)\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot([x[\"cv_results\"] for x in results_by_strategy])\n",
    "    ax.set_xticklabels([x[\"name\"] for x in results_by_strategy])\n",
    "    plt.show()\n",
    "    \n",
    "    # order the models by the mean auc\n",
    "    results_by_strategy.sort(key=lambda x: x[\"mean\"], reverse=True)\n",
    "    print([(x[\"name\"], x[\"mean\"]) for x in results_by_strategy])\n",
    "    \n",
    "    results[NA_strategy] = results_by_strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results sorted by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_sorted = [(na_method, algorithm[\"name\"], algorithm[\"mean\"]) for na_method in results for algorithm in results[na_method]]\n",
    "results_sorted.sort(key=lambda x: x[2], reverse=True)\n",
    "results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSupervisedAlgorithmsDefault(inf, sup):\n",
    "    plt.figure(figsize=(13, 7), dpi=500)\n",
    "    \n",
    "    # x axis\n",
    "    labels = [na_method for na_method in results]\n",
    "    labels.sort()\n",
    "    plt.xticks(np.arange(len(labels)), labels, rotation='vertical')\n",
    "    plt.ylim(inf, sup)\n",
    "    \n",
    "    # legend:\n",
    "    algorithm_names = [x[\"name\"] for x in results[\"01-zero\"]] \n",
    "    \n",
    "    [plt.plot([[x[\"mean\"] for x in results[na_method] if x[\"name\"] == alg_name] for na_method in sorted(results)],\n",
    "              label = alg_name) for alg_name in algorithm_names]\n",
    "    \n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('NA-filling method')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "plotSupervisedAlgorithmsDefault(0.69, 1)\n",
    "plotSupervisedAlgorithmsDefault(0.9, 1)\n",
    "plotSupervisedAlgorithmsDefault(0.99, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on this plot, we decided to tune XGB and LDA and use 07-spec-mean and 09-spec-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotConfusionMatrixBestXGB():\n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    train, test = kfold.split(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "\n",
    "    lol = xgb.XGBClassifier().fit(datasets[\"07-spec-mean\"][\"train\"].iloc[train[0]], Y_train[train[0]])\n",
    "\n",
    "    train1_pred = lol.predict(datasets[\"07-spec-mean\"][\"train\"].iloc[train[1]])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[train[1]], train1_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    \n",
    "plotConfusionMatrixBestXGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotConfusionMatrixBestLDA():\n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    train, test = kfold.split(datasets[\"09-spec-min\"][\"train\"], Y_train)\n",
    "\n",
    "    lol = xgb.XGBClassifier().fit(datasets[\"09-spec-min\"][\"train\"].iloc[train[0]], Y_train[train[0]])\n",
    "\n",
    "    train1_pred = lol.predict(datasets[\"09-spec-min\"][\"train\"].iloc[train[1]])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[train[1]], train1_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    \n",
    "plotConfusionMatrixBestLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Tuning of the best models\n",
    "#### Based on this plot, we decided to tune LDA and XGB\n",
    "### Tuning XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "def modelfit(alg, train_predictors, train_target, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_predictors.values, label=train_target.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_predictors, train_target, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(train_predictors)\n",
    "    dtrain_predprob = alg.predict_proba(train_predictors)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_target.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_target, dtrain_predprob))\n",
    "                    \n",
    "    #feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    #feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    #plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "def tuneXGB1():\n",
    "    xgb1 = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5, # This should be between 3-10\n",
    "        min_child_weight=1, # A smaller value is chosen because it is a highly imbalanced class problem\n",
    "        gamma=0,\n",
    "        subsample=0.8, # Typical values range between 0.5-0.9.\n",
    "        colsample_bytree=0.8, # Typical values range between 0.5-0.9.\n",
    "        objective= 'binary:logistic',\n",
    "        #nthread=4,\n",
    "        scale_pos_weight=1, # Because of high class imbalance\n",
    "        seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This last result seems too good to be truth?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB2():\n",
    "    param_test1 = {\n",
    "        'max_depth': np.arange(3,10,2),\n",
    "        'min_child_weight': np.arange(1,6,2)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=5,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### {'max_depth': 7, 'min_child_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB3():\n",
    "    param_test1 = {\n",
    "        'max_depth': [6,7,8],\n",
    "        'min_child_weight': [1,2,3]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=5,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB4():\n",
    "    param_test1 = {\n",
    "        'gamma':[i/10.0 for i in np.arange(0,5)]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gamma = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB5():\n",
    "    xgb1 = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=7, # This should be between 3-10\n",
    "        min_child_weight=1, # A smaller value is chosen because it is a highly imbalanced class problem\n",
    "        gamma=0,\n",
    "        subsample=0.8, # Typical values range between 0.5-0.9.\n",
    "        colsample_bytree=0.8, # Typical values range between 0.5-0.9.\n",
    "        objective= 'binary:logistic',\n",
    "        #nthread=4,\n",
    "        scale_pos_weight=1, # Because of high class imbalance\n",
    "        seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB6():\n",
    "    param_test1 = {\n",
    "     'subsample': np.arange(0.6, 1.0, 0.1),\n",
    "     'colsample_bytree': np.arange(0.6, 1.0, 0.1)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample = 0.6 and colsample_bytree = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB7():\n",
    "    param_test1 = {\n",
    "     'subsample': np.arange(0.55, 0.7, 0.05),\n",
    "     'colsample_bytree': np.arange(0.85, 1.0, 0.05)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### better tuned: subsample=0.55 and colsample_bytree=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB8():\n",
    "    param_test1 = {\n",
    "     'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reg alpha = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tuneXGB9():    \n",
    "    xgb1 = xgb.XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=7,\n",
    "                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "                      reg_alpha=1e-5, objective= 'binary:logistic', scale_pos_weight=1, seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneLDA():\n",
    "    param_test1 = [{\"solver\": [\"svd\"], \"n_components\": np.arange(1,len(X_train.columns) - 1)},\n",
    "                   {\"solver\": [\"lsqr\", \"eigen\"], \"n_components\": np.arange(1,len(X_train.columns) - 1), \"shrinkage\": [\"auto\"]}]\n",
    "        \n",
    "    gsearch1 = GridSearchCV(estimator=LinearDiscriminantAnalysis(), \n",
    "                            param_grid = param_test1, scoring='roc_auc', cv=10)\n",
    "    \n",
    "    fit = gsearch1.fit(datasets[\"09-spec-min\"][\"train\"], Y_train)\n",
    "    return(fit)\n",
    "    \n",
    "bestLDAfit = tuneLDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestLDAfit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestLDAfit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluatingBestLDA():\n",
    "    kfold = mds.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
    "    model = LinearDiscriminantAnalysis(n_components = 1, shrinkage = \"auto\", solver=\"lsqr\")\n",
    "    \n",
    "    cv_results = mds.cross_val_score(model, datasets[\"09-spec-min\"][\"train\"], Y_train, cv=kfold, scoring=\"roc_auc\")\n",
    "    print(cv_results.mean())\n",
    "    \n",
    "evaluatingBestLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Unsupervised Anomaly Detection Methods\n",
    "We decided to try LOF and see how it goes. We used our implementation from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lof_pal as lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makePredictonsLOF():\n",
    "    outliers = []\n",
    "    \n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    sets,_ = kfold.split(datasets[\"07-spec-mean\"][\"train\"], Y_train)                                      \n",
    "    \n",
    "    # Train with only positive examples:\n",
    "    l = lof.LOF(datasets[\"07-spec-mean\"][\"train\"].iloc[sets[0]][Y_train[sets[0]] != 1], 3)\n",
    "    \n",
    "    Y_pred = [1 if x > 1.2 else 0 for x in l.predict(datasets[\"07-spec-mean\"][\"train\"].iloc[sets[1]])]\n",
    "        \n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[sets[1]], Y_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    #return Y_pred\n",
    "        \n",
    "    \n",
    "makePredictonsLOF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose algorithm\n",
    "def makeSubmissionKaggle(NA_strategy, algorithm):\n",
    "    print(\"Submiting using \\\"%s\\\"\" % (NA_strategy))\n",
    "\n",
    "    algorithm.fit(datasets[NA_strategy][\"train\"], Y_train)\n",
    "    Y_pred = algorithm.predict(datasets[NA_strategy][\"test\"])\n",
    "    Y_pred = Y_pred.astype(int)\n",
    "\n",
    "    # save data to CSV\n",
    "    saveDataToCSV(Y_pred)\n",
    "    \n",
    "#makeSubmissionKaggle(\"07-spec-mean\", xgb.XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=7,\n",
    "#                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "#                      reg_alpha=1e-5, objective= 'binary:logistic', scale_pos_weight=1, seed=2))\n",
    "\n",
    "#makeSubmissionKaggle(\"09-spec-min\", LinearDiscriminantAnalysis(n_components = 1, shrinkage = \"auto\", solver=\"lsqr\"))\n",
    "makeSubmissionKaggle(\"10-spec-max\", AdaBoostClassifier())"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
