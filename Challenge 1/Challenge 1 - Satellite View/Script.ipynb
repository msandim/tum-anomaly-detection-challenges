{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#read from csv\n",
    "test_df = pd.read_csv(\"sat-test-data.csv.dat\", header=None)\n",
    "train_df = pd.read_csv(\"sat-train.csv.dat\", header =None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_df\n",
    "train_df\n",
    "Y_train = train_df[len(train_df.columns)-1]\n",
    "X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "num_rows_X_train = X_train[0].count()\n",
    "num_columns_X_train = len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Utils Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Function Save Data To CSV\n",
    "######################################\n",
    "\n",
    "def saveDataToCSV(Y_pred):\n",
    "    id_list = range(1, len(Y_pred)+1)\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": id_list,\n",
    "        \"Expected\": Y_pred\n",
    "    })\n",
    "    submission = submission[['Id', 'Expected']]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy.ma as ma\n",
    "import math\n",
    "\n",
    "######################################\n",
    "# Strategies based on constants\n",
    "######################################\n",
    "\n",
    "#replace by zero - black colour in RGB\n",
    "def replaceByZero(df):\n",
    "    return np.nan_to_num(df)\n",
    "\n",
    "#replace by 255 - white colour in RGB\n",
    "def replaceBy255(df):\n",
    "    return df.fillna(255)\n",
    "\n",
    "#####################################\n",
    "# Strategies based on columns values\n",
    "#####################################\n",
    "\n",
    "#column minimum\n",
    "def replaceByColumnMinimum(df):\n",
    "    return df.fillna(df.min())\n",
    "\n",
    "#column maximum\n",
    "def replaceByColumnMaximum(df):\n",
    "    return df.fillna(df.max())\n",
    "\n",
    "#column mean\n",
    "def replaceByColumnMean(df):\n",
    "    return df.fillna(df.mean())\n",
    "\n",
    "#column median\n",
    "def replaceByColumnMedian(df):\n",
    "    return df.fillna(df.median())\n",
    "\n",
    "#####################################\n",
    "# Strategies based on rows values\n",
    "#####################################\n",
    "\n",
    "# in the analysis of the rows we have to take into account that each four consecutive rows describe a pixel.\n",
    "# Each of these four rows stands for: Red, Green, IR, IR. |R|G|IR1|IR2|\n",
    "# These values refer to different things, and therefore are analysed independently.\n",
    "# this will allow to consider more the specificities of the problem\n",
    "\n",
    "# divide the data into four datasets, corresponding to each type of values \n",
    "\n",
    "#general function to gather columns for each type |R|G|IR1|IR2| using the mod operator\n",
    "def separatePixelColumns(position, df):\n",
    "    indexes = range(0, num_columns_X_train-1)\n",
    "    indexes = [x for x in indexes if x % 4 == position]\n",
    "    df_p_attribute = df.iloc[:,indexes]\n",
    "    return df_p_attribute\n",
    "\n",
    "#general function to fill missing values based on the rows\n",
    "#Note that does not make sense to consider the four values |R|G|IR1|IR2|, because they refer to different properties\n",
    "def fillMissingValuesByRow(df, function):\n",
    "    for index, row in df.iterrows():\n",
    "        value_without_nan = function(row)\n",
    "        nan_positions = row.isnull()\n",
    "        row[nan_positions] = value_without_nan\n",
    "    return df\n",
    "\n",
    "#row spectral mean\n",
    "def replaceByRowMean(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmean)\n",
    "\n",
    "#row spectral median\n",
    "def replaceByRowMedian(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmedian)\n",
    "\n",
    "#row spectral minimum\n",
    "def replaceByRowMinimum(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmin)\n",
    "\n",
    "#row spectral maximum\n",
    "def replaceByRowMaximum(df):\n",
    "    return fillMissingValuesByRow(df, np.nanmax)\n",
    "\n",
    "########################################\n",
    "# Strategies based on data distribution\n",
    "########################################\n",
    "\n",
    "#consider the distribution of the spectral values of each type. \n",
    "#Get random value from the spectral values of same type\n",
    "\n",
    "    \n",
    "def getRandomNumberFromDataframe(df):\n",
    "    import random\n",
    "    while True:\n",
    "        row = df.sample(1)\n",
    "        values = row.values[0]\n",
    "        value = random.choice(values)\n",
    "        if not math.isnan(value):\n",
    "            break\n",
    "    return value\n",
    "        \n",
    "def fillMissingValuesWithDistribution(df):\n",
    "    for index, row in df.iterrows():\n",
    "        nan_positions = row.isnull()\n",
    "        for i in range(len(nan_positions)): \n",
    "            if nan_positions.iloc[i] == True:\n",
    "                value = getRandomNumberFromDataframe(df)\n",
    "                row.iloc[i] = value\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################%%%\n",
    "# fill Missing Values Considering Spectral\n",
    "#########################################%%%\n",
    "\n",
    "def fillMissingValuesBySpectral(df, function):\n",
    "\n",
    "    #generate four new datasets with the columns of each type\n",
    "    df_p_attribute_R = separatePixelColumns(0, df)\n",
    "    df_p_attribute_G = separatePixelColumns(1, df)\n",
    "    df_p_attribute_IR1 = separatePixelColumns(2, df)\n",
    "    df_p_attribute_IR2 = separatePixelColumns(3, df)\n",
    "\n",
    "    #apply function to each of the 4 datasets\n",
    "    df_p_attribute_R = function(df_p_attribute_R)\n",
    "    df_p_attribute_G = function(df_p_attribute_G)\n",
    "    df_p_attribute_IR1 = function(df_p_attribute_IR1)\n",
    "    df_p_attribute_IR2 = function(df_p_attribute_IR2)\n",
    "\n",
    "    df = pd.concat(\n",
    "                        [df_p_attribute_R, \n",
    "                         df_p_attribute_G,\n",
    "                         df_p_attribute_IR1,\n",
    "                         df_p_attribute_IR2], \n",
    "                        axis=1\n",
    "                        )\n",
    "    return df\n",
    "\n",
    "#######################\n",
    "#   CHAMADAS\n",
    "#######################\n",
    "\n",
    "#X_train = replaceByZero(X_train)\n",
    "#test_df = replaceByZero(test_df)\n",
    "\n",
    "#X_train = replaceBy255(X_train)\n",
    "#test_df = replaceBy255(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMinimum(X_train)\n",
    "#test_df = replaceByColumnMinimum(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMaximum(X_train)\n",
    "#test_df = replaceByColumnMaximum(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMean(X_train)\n",
    "#test_df = replaceByColumnMean(test_df)\n",
    "\n",
    "#X_train = replaceByColumnMedian(X_train)\n",
    "#test_df = replaceByColumnMedian(test_df)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMean)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMean)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMedian)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMedian)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMinimum)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMinimum)\n",
    "\n",
    "#X_train = fillMissingValuesBySpectral(X_train, replaceByRowMaximum)\n",
    "#test_df = fillMissingValuesBySpectral(test_df, replaceByRowMaximum)\n",
    "\n",
    "X_train = fillMissingValuesBySpectral(X_train, fillMissingValuesWithDistribution)\n",
    "test_df = fillMissingValuesBySpectral(test_df, fillMissingValuesWithDistribution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Normalization\n",
    "\n",
    "There is no need of normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Choose Model\n",
    "\n",
    "Nota: adicionar neural networks - new in v 0.18\n",
    "\n",
    "First goal: discover which type of analyses works better\n",
    "Second Goal: tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.567664 (0.406939)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA: 0.605677 (0.398549)\n",
      "KNN: 0.601620 (0.396995)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART: 0.558652 (0.393427)\n",
      "NB: 0.602192 (0.413618)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Paula\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM: 0.000000 (0.000000)\n"
     ]
    }
   ],
   "source": [
    "#inspired in http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# prepare data\n",
    "\n",
    "#Y_train = train_df[:,-1]\n",
    "#X_train = train_df[:,:-1]\n",
    "\n",
    "#Y_train = train_df[len(train_df.columns)-1]\n",
    "#X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "\n",
    "# prepare configuration for cross validation test harness\n",
    "num_folds = 10\n",
    "num_instances = len(X_train)\n",
    "seed = 7\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('NN', MLPClassifier(alpha=1))) \n",
    "models.append(('RF', RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)))\n",
    "models.append(('AB', AdaBoostClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'f1' # try with 'roc_auc', f1'\n",
    "\n",
    "for name, model in models:\n",
    "    kfold = cross_validation.KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n",
    "    cv_results = cross_validation.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose algorithm\n",
    "\n",
    "funtion = AdaBoostClassifier\n",
    "\n",
    "result = funtion()\n",
    "\n",
    "result.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = result.predict(test_df)\n",
    "\n",
    "result.score(X_train, Y_train)\n",
    "\n",
    "Y_pred = Y_pred.astype(int)\n",
    "\n",
    "# save data to CSV\n",
    "\n",
    "saveDataToCSV(Y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "- Poderíamos tentar fazer alguma coisa a ver com anomaly, porque até agora é uma análise muito geral:\n",
    "-- Pedir frequências das duas classes.\n",
    "-- Pedir um gráfico com as distribuições dos valores para cada espectro.\n",
    "-- Ver resultado quando se lida com os Missing values pela estratégia da distribuição\n",
    "-- ter o cuidado de ver se os resultados não estão a ser influenciados porque o nosso algoritmo diz sempre a mesma classe: ver precision e recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ReadData\n",
    "#XTrain, YTrain, rawTestData = importDataFromFiles()\n",
    "\n",
    "#handle missing values\n",
    "\n",
    "#for each different strategy for the missing values check which is the best model in the, get values for accuracy, auc, F1\n",
    "#check both on the K fold, and on the test dataset\n",
    "\n",
    "#pick the best strategy for missing values and  the best model and submit to kaggle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Próximos Passos:\n",
    "\n",
    "- criar forma de comparar diferentes estratégias de missing vs diferentes algoritmos, gráfico?\n",
    "- ver frequências das duas classes\n",
    "- tentar com algoritmo de anomalia\n",
    "- escolher melhor algoritmo entre todos (até agora adaboost)\n",
    "- fazer tuning ao algoritmo\n",
    "- FIM!\n",
    "\n",
    "- fazer apresentação e report:\n",
    "\n",
    "até agora a evolução dos trabalhos a colocar no report será:\n",
    "- pipeline testing and submission with 0 + adaboost\n",
    "- improvement in the strategies for missing values - from the different ones the row minimum proved to improve results\n",
    "- analysis of algorithms + anomaly detection algorithms\n",
    "- tuning\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
