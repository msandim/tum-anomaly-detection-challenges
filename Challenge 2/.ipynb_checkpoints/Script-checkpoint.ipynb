{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Challenge 2\n",
    "## Miguel Sandim and Paula Fortuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Library Imports & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Random libraries and seeds:\n",
    "import random\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "######################################\n",
    "# Function Save Data To CSV\n",
    "######################################\n",
    "\n",
    "def saveDataToCSV(Y_pred):\n",
    "    id_list = range(0, len(Y_pred))\n",
    "    submission = pd.DataFrame({\n",
    "        \"Id\": id_list,\n",
    "        \"Expected\": Y_pred\n",
    "    })\n",
    "    submission = submission[['Id', 'Expected']]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Solve format problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 -  yelp_data_train.dat and yelp_data_test.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a text editor (e. g. sublime) use regex.\n",
    "\n",
    "1) Replace \" by \"\"\n",
    "\n",
    "2) Surround text field with \"\n",
    "To match the first one use this (dont forget to remove the one that appears also in the begining of the sentence, and the one in the header):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "^[^;]*;[^;]*;[^;]*;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to find the last:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    ";[^;]*;[^;]*;[^;]*;[^;]*;[^;]*;[^;]*$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) check if all the lines match the refered structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "^[^;]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Remove random newlines that appear in the rows and make new instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 - yelp_data_reviewer.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) ; caracter removed from fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) smiles removed from fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 hotel file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) removal of ; in the link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load data (finally!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read from csv\n",
    "\n",
    "train_df = pd.read_csv(\"data/yelp_data_train.dat\", sep = ';', encoding = 'utf-8')\n",
    "test_df = pd.read_csv(\"data/yelp_data_test.dat\", sep = ';', encoding = 'utf-8')\n",
    "reviewers_df = pd.read_csv(\"data/yelp_data_reviewer.dat\", sep = ';', encoding = 'utf-8')\n",
    "hotels_df = pd.read_csv(\"data/yelp_data_hotel.dat\", sep = ';', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N    2516\n",
       "Y     392\n",
       "Name: fake, dtype: int64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"fake\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13480055020632736"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "392/(392 + 2516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About 13% of our dataset are anomalous cases. We also checked that each reviewer only reviewed each hotel once, at maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as skpre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Solve Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>reviewID</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewContent</th>\n",
       "      <th>rating</th>\n",
       "      <th>usefulCount</th>\n",
       "      <th>coolCount</th>\n",
       "      <th>funnyCount</th>\n",
       "      <th>fake</th>\n",
       "      <th>hotelID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9/16/2010</td>\n",
       "      <td>Ol</td>\n",
       "      <td>nf3q2h-kSQoZK2jBY92FOg</td>\n",
       "      <td>If you are considering staying here, watch thi...</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>N</td>\n",
       "      <td>tQfLGoolUMu2J0igcWcoZg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/5/2010</td>\n",
       "      <td>i4HIAcNTjabdpG1K4F5Q2g</td>\n",
       "      <td>Sb3DJGdZ4Rq__CqxPbae-g</td>\n",
       "      <td>This place is disgusting, absolutely horrible,...</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>N</td>\n",
       "      <td>tQfLGoolUMu2J0igcWcoZg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8/9/2010</td>\n",
       "      <td>veKKNAaSKWj8os</td>\n",
       "      <td>nR7zLyFOlzAYqmzgJ3DtXg</td>\n",
       "      <td>Disgusting!!! Â There is literally duct tape ho...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>tQfLGoolUMu2J0igcWcoZg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8/11/2012</td>\n",
       "      <td>6c-ZiQkHXtp1n6VfiKDQ3g</td>\n",
       "      <td>747lP4p8dUD6RTkcsIaSGg</td>\n",
       "      <td>This hotel came up on Hotwire for $108 a night...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/9/2012</td>\n",
       "      <td>POWQ6FuUf3oe2ZkhmHvciA</td>\n",
       "      <td>Ij5t6VdwtasSkrpp9uAbKg</td>\n",
       "      <td>Good location, really run down. I am surprised...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6/19/2012</td>\n",
       "      <td>QBynYcLgIgtAd-YfnrrAtA</td>\n",
       "      <td>hSERzClUe57bCw3nCp4plA</td>\n",
       "      <td>Beautiful lobby. The rest is a dump. The eleva...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9/14/2012</td>\n",
       "      <td>ELY3TK</td>\n",
       "      <td>OMm2VcGks3QL0p0n3_kPFw</td>\n",
       "      <td>Stayed here when I went to Chicago for a weddi...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3/20/2012</td>\n",
       "      <td>uWKWYb5vDpeDGEAZUc192g</td>\n",
       "      <td>yevHGEUQQmnVlBXIrJ885A</td>\n",
       "      <td>I bleed SPG loyalty blood to the point where I...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3/3/2012</td>\n",
       "      <td>hkt7Dnr7kRnLLd9pm-fxDw</td>\n",
       "      <td>Lql1_3zeGlny_Tgq4MI6Fg</td>\n",
       "      <td>I stayed here a couple of times in 2011, as th...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6/18/2012</td>\n",
       "      <td>ZlexD7XvkqH8yve4zCAR7g</td>\n",
       "      <td>RtyDimVdIBwjGdQr0dti1w</td>\n",
       "      <td>This is an older property, so the decor is dat...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3/13/2012</td>\n",
       "      <td>Rw1JmyRyyjoACCUFvmS9kQ</td>\n",
       "      <td>hCIJT7tIhPX_YZBCPhYhMg</td>\n",
       "      <td>Small Rooms....one elevator that takes forever...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5/24/2010</td>\n",
       "      <td>O9chyjQi5</td>\n",
       "      <td>nKgjmPhPPiJ8BL97dO76XA</td>\n",
       "      <td>Great location, terribly outdated. Feels like ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10/25/2010</td>\n",
       "      <td>x4FHvju16JpVa3ihzIwQvw</td>\n",
       "      <td>IhKctrZ3BtJkfpf0qO-8mQ</td>\n",
       "      <td>My husband and I came to Chicago for a week an...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2/13/2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yoB_PYQHjnPjh78ATA0Jgw</td>\n",
       "      <td>Stayed here over the Jan 15th weekend. Â The lo...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8/9/2011</td>\n",
       "      <td>9CPWfP7Ibj-2TthBN</td>\n",
       "      <td>b8B2b2q_Lcactxp8xr-jvg</td>\n",
       "      <td>Breakfast no longer free, but the wifi is. The...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>9/21/2010</td>\n",
       "      <td>4sIRV-M-nrhyU5wHJSznrA</td>\n",
       "      <td>y5ptsWmvGEAftOQaiFhBcg</td>\n",
       "      <td>I can't imagine paying full price to stay here...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6/17/2009</td>\n",
       "      <td>s3V-CRnWtlHc2goC7xIAEA</td>\n",
       "      <td>QTJCwSaCfBz3T944SOiHBQ</td>\n",
       "      <td>For the money.. it was a great hotel. Clean, g...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4/4/2012</td>\n",
       "      <td>A5WJkZECjZVwSoV8Uxciww</td>\n",
       "      <td>_h5MGqTBM8J0tuAfbEvVDg</td>\n",
       "      <td>Great location for walking to any part of down...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7/8/2011</td>\n",
       "      <td>xnZtuSIepywIkSC1uGYzGg</td>\n",
       "      <td>s6tSJ1NQMSU59lsCPYzzwQ</td>\n",
       "      <td>Excellent location right down the street from ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4/4/2011</td>\n",
       "      <td>FRM7-hER5S9wMf0IzciDcg</td>\n",
       "      <td>0oA4l1ob50aIZ9x9Kxobqg</td>\n",
       "      <td>Great location for a good price. Book it direc...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8/23/2010</td>\n",
       "      <td>0fLzbPuBnOMKzVnmxqz-1w</td>\n",
       "      <td>3KM5g9hyaR-uE92WfMU6Cw</td>\n",
       "      <td>Another Travelzoo deal at an awesome price of ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8/9/2008</td>\n",
       "      <td>MGcMncJSQQ7zzE</td>\n",
       "      <td>Nybw4RhZvdexOR-XgXD4DQ</td>\n",
       "      <td>For the love of God, whatever you do, DO NOT t...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4/24/2011</td>\n",
       "      <td>8a8MaeJ99mi3WTUZYDYL2Q</td>\n",
       "      <td>SKgjcQz0TNg8LKqlJwxjfg</td>\n",
       "      <td>Decent price for downtown Chicago over a weeke...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10/3/2009</td>\n",
       "      <td>K8HSCX-GCOcF8VUsAtMSGQ</td>\n",
       "      <td>tdE3__i2otI_nL3M3sy0MQ</td>\n",
       "      <td>Some rooms may reveal the building's age, but ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4/17/2011</td>\n",
       "      <td>2rAeHez1ZzyHvL1u2DIJsw</td>\n",
       "      <td>fOTzYtBF4lJTwDIMfCAeyQ</td>\n",
       "      <td>If you want a reasonably priced, clean, non-ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>7/17/2010</td>\n",
       "      <td>07mEVsw7wUKuUOH</td>\n",
       "      <td>Uu-qEGsSb72ngIQUF85rDQ</td>\n",
       "      <td>I arrived at the Tremont this past weekend for...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10/26/2009</td>\n",
       "      <td>Og0iVk</td>\n",
       "      <td>ptgFdw9WzPQx16w37XxsPQ</td>\n",
       "      <td>I have a fondness for older hotels that are a ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>33Xc1Bk_gkSY5xb2doQ7Ng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11/23/2011</td>\n",
       "      <td>JKcDTRG0dYs23kj1NeOB5A</td>\n",
       "      <td>zyk-YPhtFZK6kkbpzEKrWw</td>\n",
       "      <td>Well this was an interesting place. Â I went to...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>2nnXespKBBNtDQTtrumNFg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4/5/2012</td>\n",
       "      <td>6h8rBu1K3AxUiQE8biaTIg</td>\n",
       "      <td>T4LT4dPTTTVZocPhwrJboQ</td>\n",
       "      <td>The card key never worked. Â I even got an extr...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>2nnXespKBBNtDQTtrumNFg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3/24/2012</td>\n",
       "      <td>4uniW9GJpi56lRUyJFU-OQ</td>\n",
       "      <td>HfypTYNqexsCBuRBQFhKTA</td>\n",
       "      <td>I won't go into details but basically employee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>2nnXespKBBNtDQTtrumNFg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2878</th>\n",
       "      <td>5/1/2007</td>\n",
       "      <td>ELidp04R7jwXfXV5mFW1XQ</td>\n",
       "      <td>_f9NyNygDasxm4x_8K0FMg</td>\n",
       "      <td>Boo Marriott. You may have comfy beds but hosp...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Gh4Tb2qCBUCOd5r7jqKnGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>7/14/2012</td>\n",
       "      <td>nJh1lZINVD5SAW2ecQir7A</td>\n",
       "      <td>aAnY0oKxg4WEcYsC9hw3cQ</td>\n",
       "      <td>I attended a week-long training in their confe...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>BISUDalmPulSzHvsO3PhDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>3/5/2012</td>\n",
       "      <td>QkDj-K2</td>\n",
       "      <td>YfsNifnBJYJ1aq05eC-u3g</td>\n",
       "      <td>I am torn with how I want to rate this hotel. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>BISUDalmPulSzHvsO3PhDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2881</th>\n",
       "      <td>9/14/2010</td>\n",
       "      <td>TFKtb8DG7yTAxkntYCRMTQ</td>\n",
       "      <td>uqC3ll6vCfqFalbWnPi-Qg</td>\n",
       "      <td>Rooms are kinda old. Service is pretty mediocr...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>BISUDalmPulSzHvsO3PhDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2882</th>\n",
       "      <td>1/26/2010</td>\n",
       "      <td>QYD-28qlQRmlAe7G2FbNUA</td>\n",
       "      <td>mzO44cSLjgjo-TUYUqPyHA</td>\n",
       "      <td>I agree with the first reviewer Celeste. I had...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>BISUDalmPulSzHvsO3PhDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2883</th>\n",
       "      <td>8/3/2011</td>\n",
       "      <td>ZjhC8PHPq5RGocFrXTYcWw</td>\n",
       "      <td>oa027MtAk0PV7Tom9K8xHQ</td>\n",
       "      <td>For the price I got this room for it was Great...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>BISUDalmPulSzHvsO3PhDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>5/27/2010</td>\n",
       "      <td>u5gKtqSW4os8H1OImGEkEw</td>\n",
       "      <td>b_54V-mRPPHIwj0wFWby-g</td>\n",
       "      <td>I once found a key on the sidewalk near here, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>N</td>\n",
       "      <td>bem-1CTpTNpArpCtThTmFw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>2/3/2009</td>\n",
       "      <td>VlJdOpIVNevAbBeESTofeQ</td>\n",
       "      <td>WauJzu-aZSJZuCLkLWFkag</td>\n",
       "      <td>I was once told by the talent buyer at the Log...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>bem-1CTpTNpArpCtThTmFw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>10/4/2009</td>\n",
       "      <td>1bY3wvqJD3zOUx6b-HOb8g</td>\n",
       "      <td>OBz87AmKkY-RKP7dFQFugQ</td>\n",
       "      <td>Where do I begin?? There are so many horrible ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>bem-1CTpTNpArpCtThTmFw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>9/4/2009</td>\n",
       "      <td>BMQbKdF_m2gGeY6hr9Fv1Q</td>\n",
       "      <td>Dv3r6dxp6N7B2S4stwS4dg</td>\n",
       "      <td>This is obviously a place for junkies and hook...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>bem-1CTpTNpArpCtThTmFw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>12/25/2011</td>\n",
       "      <td>M9jtbdwJYyxwWWufBaw3EA</td>\n",
       "      <td>-2aZoH_YaC-1s1BFzo7GcA</td>\n",
       "      <td>Over priced, Â nickle and dime you for everythi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>g51qDl6fQhgat-kFTrcbug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>6/24/2011</td>\n",
       "      <td>LuMU5Me4cAMaxmoKWUrNZg</td>\n",
       "      <td>N0pY2Nd7xTZupPd3SYUWEA</td>\n",
       "      <td>Housekeeping staff here needs lessons--dusty f...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>g51qDl6fQhgat-kFTrcbug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>11/30/2010</td>\n",
       "      <td>vE1oKqdlBCOfRDf7ObE_WQ</td>\n",
       "      <td>HWIJ4kkTMnuCTBkLmaupMA</td>\n",
       "      <td>There are too many old men trying to pick up w...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>BISUDalmPulSzHvsO3PhDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>6/28/2011</td>\n",
       "      <td>ux6ZfjP5yWHXU4zQA3P7yA</td>\n",
       "      <td>zJ2Op8FsslxF5TSem1f-zw</td>\n",
       "      <td>I used priceline to get a room at this hotel. ...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Gh4Tb2qCBUCOd5r7jqKnGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>4/16/2010</td>\n",
       "      <td>HzOVUdfy4vAXsIm_eH-Pww</td>\n",
       "      <td>2ck_k8Swtj6-48kLR6IVWA</td>\n",
       "      <td>When we got to the place where check in was ha...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Gh4Tb2qCBUCOd5r7jqKnGA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>6/14/2011</td>\n",
       "      <td>D9urHoAJiktxTfpDy84vUw</td>\n",
       "      <td>MtR3yxthVe-LoHEAXUKSGw</td>\n",
       "      <td>My boss booked two rooms here because the pric...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>h-U_YfWBK2u_Vqo8AVA4KQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>6/4/2010</td>\n",
       "      <td>TYcOMlIhPaobaHFLywRLvw</td>\n",
       "      <td>2ynZkFzXxSYli0acUGEjxA</td>\n",
       "      <td>We stayed in the motel because of it's conveni...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>h-U_YfWBK2u_Vqo8AVA4KQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>9/25/2011</td>\n",
       "      <td>ECKiMfnDzi11HdfeK0LIBw</td>\n",
       "      <td>pB_MluKOj16YTAH4ZozyNA</td>\n",
       "      <td>Would recommend this hotel to anyone. Â My wife...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>wLjR0DkA4zxu8iqbUQc0Og</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>7/27/2012</td>\n",
       "      <td>W0KFuAk6Y62JTMdJw_7hWw</td>\n",
       "      <td>cHegKQwdtrdd3Hpo1b6NsA</td>\n",
       "      <td>Fantastic location, friendly staff and nice ac...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>t5JK4OZzCuetIrJXdHLntg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>5/18/2012</td>\n",
       "      <td>t9G4R6uGohktZeHZmf3kKA</td>\n",
       "      <td>nFSimJkp5uq8FPTyY-4NNQ</td>\n",
       "      <td>This boutique hotel is in an excellent locatio...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>t5JK4OZzCuetIrJXdHLntg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>8/1/2011</td>\n",
       "      <td>f_MP8IYIvV_Sp3AgAFG0Iw</td>\n",
       "      <td>YMsf5cb9kbEj7DuPdyQ4eA</td>\n",
       "      <td>My wife and I stayed here July 30th. We arrive...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>t5JK4OZzCuetIrJXdHLntg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>7/19/2011</td>\n",
       "      <td>6mqU5bF4Iq1ZHBfknBUsYQ</td>\n",
       "      <td>JwKrfYb2BNFRZdHCFlwO1w</td>\n",
       "      <td>Very clean and super helpful staff. Â The rooms...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>t5JK4OZzCuetIrJXdHLntg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>6/16/2010</td>\n",
       "      <td>pXxaEx57oNIMqsLBqVczyg</td>\n",
       "      <td>hI8LGBnG-VupdJhsj_U3Og</td>\n",
       "      <td>My company held an event here and the staff ov...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>S_qmb0Uzm_cNoRjSvZ6y1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>4/6/2010</td>\n",
       "      <td>OTJHRZXJn8UfAJ4ljWN9sw</td>\n",
       "      <td>hU9lnr5ZAwWr0SHzg7RdHg</td>\n",
       "      <td>This boutique hotel has a very personal approa...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>S_qmb0Uzm_cNoRjSvZ6y1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>10/10/2008</td>\n",
       "      <td>IDSx8yGQKNUImss-4YUlGg</td>\n",
       "      <td>BaRtixU-lZsMr2PF4hrvqg</td>\n",
       "      <td>Wife and I stay here when we visit. Went up on...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>S_qmb0Uzm_cNoRjSvZ6y1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>8/2/2012</td>\n",
       "      <td>lFkroXzrRhmMTrnn6__tnw</td>\n",
       "      <td>ZpFB_pqaUaDmIuCXbN87Ww</td>\n",
       "      <td>All Westins are definitely nice and so was thi...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>LUmAQaRrAleKdXZd8On16Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>12/14/2011</td>\n",
       "      <td>mV2x61nyxzsKRgGbM3yuqg</td>\n",
       "      <td>twaozx1wvDOL0Z_fG9gclA</td>\n",
       "      <td>This place was awesome! Well except for the we...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>PyG0aSX3pBx0hzoSH20FnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>11/21/2009</td>\n",
       "      <td>CNwz6w426kCF5H6j-neQSg</td>\n",
       "      <td>S5L1xguLhXJWadpFXnRKqQ</td>\n",
       "      <td>My friend and I, along with our mothers, staye...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>PyG0aSX3pBx0hzoSH20FnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>11/20/2011</td>\n",
       "      <td>1eJPupHCsl-r3WUuz6QvTA</td>\n",
       "      <td>PJK4GsUItBU8JJuoAujWOA</td>\n",
       "      <td>So Surprising when we arrived, The staff was g...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>gCdjyQeE0uRKCh7mVmnZzQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>8/5/2010</td>\n",
       "      <td>xwPMoEzuvpn3J32IvTcsiQ</td>\n",
       "      <td>MdYbNl_9Hm1CybsuC6UnkQ</td>\n",
       "      <td>Noise, noise, noise! Â Unbelievable! Â Between t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>rpP9iZsT3NC-Z4pUtQGoiA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2908 rows Ã 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date                reviewID              reviewerID  \\\n",
       "0      9/16/2010                      Ol  nf3q2h-kSQoZK2jBY92FOg   \n",
       "1       2/5/2010  i4HIAcNTjabdpG1K4F5Q2g  Sb3DJGdZ4Rq__CqxPbae-g   \n",
       "2       8/9/2010          veKKNAaSKWj8os  nR7zLyFOlzAYqmzgJ3DtXg   \n",
       "3      8/11/2012  6c-ZiQkHXtp1n6VfiKDQ3g  747lP4p8dUD6RTkcsIaSGg   \n",
       "4       7/9/2012  POWQ6FuUf3oe2ZkhmHvciA  Ij5t6VdwtasSkrpp9uAbKg   \n",
       "5      6/19/2012  QBynYcLgIgtAd-YfnrrAtA  hSERzClUe57bCw3nCp4plA   \n",
       "6      9/14/2012                  ELY3TK  OMm2VcGks3QL0p0n3_kPFw   \n",
       "7      3/20/2012  uWKWYb5vDpeDGEAZUc192g  yevHGEUQQmnVlBXIrJ885A   \n",
       "8       3/3/2012  hkt7Dnr7kRnLLd9pm-fxDw  Lql1_3zeGlny_Tgq4MI6Fg   \n",
       "9      6/18/2012  ZlexD7XvkqH8yve4zCAR7g  RtyDimVdIBwjGdQr0dti1w   \n",
       "10     3/13/2012  Rw1JmyRyyjoACCUFvmS9kQ  hCIJT7tIhPX_YZBCPhYhMg   \n",
       "11     5/24/2010               O9chyjQi5  nKgjmPhPPiJ8BL97dO76XA   \n",
       "12    10/25/2010  x4FHvju16JpVa3ihzIwQvw  IhKctrZ3BtJkfpf0qO-8mQ   \n",
       "13     2/13/2011                     NaN  yoB_PYQHjnPjh78ATA0Jgw   \n",
       "14      8/9/2011       9CPWfP7Ibj-2TthBN  b8B2b2q_Lcactxp8xr-jvg   \n",
       "15     9/21/2010  4sIRV-M-nrhyU5wHJSznrA  y5ptsWmvGEAftOQaiFhBcg   \n",
       "16     6/17/2009  s3V-CRnWtlHc2goC7xIAEA  QTJCwSaCfBz3T944SOiHBQ   \n",
       "17      4/4/2012  A5WJkZECjZVwSoV8Uxciww  _h5MGqTBM8J0tuAfbEvVDg   \n",
       "18      7/8/2011  xnZtuSIepywIkSC1uGYzGg  s6tSJ1NQMSU59lsCPYzzwQ   \n",
       "19      4/4/2011  FRM7-hER5S9wMf0IzciDcg  0oA4l1ob50aIZ9x9Kxobqg   \n",
       "20     8/23/2010  0fLzbPuBnOMKzVnmxqz-1w  3KM5g9hyaR-uE92WfMU6Cw   \n",
       "21      8/9/2008          MGcMncJSQQ7zzE  Nybw4RhZvdexOR-XgXD4DQ   \n",
       "22     4/24/2011  8a8MaeJ99mi3WTUZYDYL2Q  SKgjcQz0TNg8LKqlJwxjfg   \n",
       "23     10/3/2009  K8HSCX-GCOcF8VUsAtMSGQ  tdE3__i2otI_nL3M3sy0MQ   \n",
       "24     4/17/2011  2rAeHez1ZzyHvL1u2DIJsw  fOTzYtBF4lJTwDIMfCAeyQ   \n",
       "25     7/17/2010         07mEVsw7wUKuUOH  Uu-qEGsSb72ngIQUF85rDQ   \n",
       "26    10/26/2009                  Og0iVk  ptgFdw9WzPQx16w37XxsPQ   \n",
       "27    11/23/2011  JKcDTRG0dYs23kj1NeOB5A  zyk-YPhtFZK6kkbpzEKrWw   \n",
       "28      4/5/2012  6h8rBu1K3AxUiQE8biaTIg  T4LT4dPTTTVZocPhwrJboQ   \n",
       "29     3/24/2012  4uniW9GJpi56lRUyJFU-OQ  HfypTYNqexsCBuRBQFhKTA   \n",
       "...          ...                     ...                     ...   \n",
       "2878    5/1/2007  ELidp04R7jwXfXV5mFW1XQ  _f9NyNygDasxm4x_8K0FMg   \n",
       "2879   7/14/2012  nJh1lZINVD5SAW2ecQir7A  aAnY0oKxg4WEcYsC9hw3cQ   \n",
       "2880    3/5/2012                 QkDj-K2  YfsNifnBJYJ1aq05eC-u3g   \n",
       "2881   9/14/2010  TFKtb8DG7yTAxkntYCRMTQ  uqC3ll6vCfqFalbWnPi-Qg   \n",
       "2882   1/26/2010  QYD-28qlQRmlAe7G2FbNUA  mzO44cSLjgjo-TUYUqPyHA   \n",
       "2883    8/3/2011  ZjhC8PHPq5RGocFrXTYcWw  oa027MtAk0PV7Tom9K8xHQ   \n",
       "2884   5/27/2010  u5gKtqSW4os8H1OImGEkEw  b_54V-mRPPHIwj0wFWby-g   \n",
       "2885    2/3/2009  VlJdOpIVNevAbBeESTofeQ  WauJzu-aZSJZuCLkLWFkag   \n",
       "2886   10/4/2009  1bY3wvqJD3zOUx6b-HOb8g  OBz87AmKkY-RKP7dFQFugQ   \n",
       "2887    9/4/2009  BMQbKdF_m2gGeY6hr9Fv1Q  Dv3r6dxp6N7B2S4stwS4dg   \n",
       "2888  12/25/2011  M9jtbdwJYyxwWWufBaw3EA  -2aZoH_YaC-1s1BFzo7GcA   \n",
       "2889   6/24/2011  LuMU5Me4cAMaxmoKWUrNZg  N0pY2Nd7xTZupPd3SYUWEA   \n",
       "2890  11/30/2010  vE1oKqdlBCOfRDf7ObE_WQ  HWIJ4kkTMnuCTBkLmaupMA   \n",
       "2891   6/28/2011  ux6ZfjP5yWHXU4zQA3P7yA  zJ2Op8FsslxF5TSem1f-zw   \n",
       "2892   4/16/2010  HzOVUdfy4vAXsIm_eH-Pww  2ck_k8Swtj6-48kLR6IVWA   \n",
       "2893   6/14/2011  D9urHoAJiktxTfpDy84vUw  MtR3yxthVe-LoHEAXUKSGw   \n",
       "2894    6/4/2010  TYcOMlIhPaobaHFLywRLvw  2ynZkFzXxSYli0acUGEjxA   \n",
       "2895   9/25/2011  ECKiMfnDzi11HdfeK0LIBw  pB_MluKOj16YTAH4ZozyNA   \n",
       "2896   7/27/2012  W0KFuAk6Y62JTMdJw_7hWw  cHegKQwdtrdd3Hpo1b6NsA   \n",
       "2897   5/18/2012  t9G4R6uGohktZeHZmf3kKA  nFSimJkp5uq8FPTyY-4NNQ   \n",
       "2898    8/1/2011  f_MP8IYIvV_Sp3AgAFG0Iw  YMsf5cb9kbEj7DuPdyQ4eA   \n",
       "2899   7/19/2011  6mqU5bF4Iq1ZHBfknBUsYQ  JwKrfYb2BNFRZdHCFlwO1w   \n",
       "2900   6/16/2010  pXxaEx57oNIMqsLBqVczyg  hI8LGBnG-VupdJhsj_U3Og   \n",
       "2901    4/6/2010  OTJHRZXJn8UfAJ4ljWN9sw  hU9lnr5ZAwWr0SHzg7RdHg   \n",
       "2902  10/10/2008  IDSx8yGQKNUImss-4YUlGg  BaRtixU-lZsMr2PF4hrvqg   \n",
       "2903    8/2/2012  lFkroXzrRhmMTrnn6__tnw  ZpFB_pqaUaDmIuCXbN87Ww   \n",
       "2904  12/14/2011  mV2x61nyxzsKRgGbM3yuqg  twaozx1wvDOL0Z_fG9gclA   \n",
       "2905  11/21/2009  CNwz6w426kCF5H6j-neQSg  S5L1xguLhXJWadpFXnRKqQ   \n",
       "2906  11/20/2011  1eJPupHCsl-r3WUuz6QvTA  PJK4GsUItBU8JJuoAujWOA   \n",
       "2907    8/5/2010  xwPMoEzuvpn3J32IvTcsiQ  MdYbNl_9Hm1CybsuC6UnkQ   \n",
       "\n",
       "                                          reviewContent  rating  usefulCount  \\\n",
       "0     If you are considering staying here, watch thi...       1            8   \n",
       "1     This place is disgusting, absolutely horrible,...       3           11   \n",
       "2     Disgusting!!! Â There is literally duct tape ho...       1            1   \n",
       "3     This hotel came up on Hotwire for $108 a night...       4            2   \n",
       "4     Good location, really run down. I am surprised...       2            0   \n",
       "5     Beautiful lobby. The rest is a dump. The eleva...       1            0   \n",
       "6     Stayed here when I went to Chicago for a weddi...       3            2   \n",
       "7     I bleed SPG loyalty blood to the point where I...       1            1   \n",
       "8     I stayed here a couple of times in 2011, as th...       3            0   \n",
       "9     This is an older property, so the decor is dat...       1            0   \n",
       "10    Small Rooms....one elevator that takes forever...       2            0   \n",
       "11    Great location, terribly outdated. Feels like ...       2            2   \n",
       "12    My husband and I came to Chicago for a week an...       4            1   \n",
       "13    Stayed here over the Jan 15th weekend. Â The lo...       4            0   \n",
       "14    Breakfast no longer free, but the wifi is. The...       4            1   \n",
       "15    I can't imagine paying full price to stay here...       2            0   \n",
       "16    For the money.. it was a great hotel. Clean, g...       3            0   \n",
       "17    Great location for walking to any part of down...       4            0   \n",
       "18    Excellent location right down the street from ...       4            0   \n",
       "19    Great location for a good price. Book it direc...       4            0   \n",
       "20    Another Travelzoo deal at an awesome price of ...       2            0   \n",
       "21    For the love of God, whatever you do, DO NOT t...       4            0   \n",
       "22    Decent price for downtown Chicago over a weeke...       2            0   \n",
       "23    Some rooms may reveal the building's age, but ...       4            0   \n",
       "24    If you want a reasonably priced, clean, non-ch...       3            0   \n",
       "25    I arrived at the Tremont this past weekend for...       2            2   \n",
       "26    I have a fondness for older hotels that are a ...       4            0   \n",
       "27    Well this was an interesting place. Â I went to...       3            1   \n",
       "28    The card key never worked. Â I even got an extr...       1            1   \n",
       "29    I won't go into details but basically employee...       1            1   \n",
       "...                                                 ...     ...          ...   \n",
       "2878  Boo Marriott. You may have comfy beds but hosp...       1            2   \n",
       "2879  I attended a week-long training in their confe...       4            1   \n",
       "2880  I am torn with how I want to rate this hotel. ...       4            3   \n",
       "2881  Rooms are kinda old. Service is pretty mediocr...       3            0   \n",
       "2882  I agree with the first reviewer Celeste. I had...       2            2   \n",
       "2883  For the price I got this room for it was Great...       3            1   \n",
       "2884  I once found a key on the sidewalk near here, ...       1            4   \n",
       "2885  I was once told by the talent buyer at the Log...       1            3   \n",
       "2886  Where do I begin?? There are so many horrible ...       1            0   \n",
       "2887  This is obviously a place for junkies and hook...       5            0   \n",
       "2888  Over priced, Â nickle and dime you for everythi...       1            0   \n",
       "2889  Housekeeping staff here needs lessons--dusty f...       2            0   \n",
       "2890  There are too many old men trying to pick up w...       2            0   \n",
       "2891  I used priceline to get a room at this hotel. ...       4            0   \n",
       "2892  When we got to the place where check in was ha...       1            0   \n",
       "2893  My boss booked two rooms here because the pric...       1            0   \n",
       "2894  We stayed in the motel because of it's conveni...       1            0   \n",
       "2895  Would recommend this hotel to anyone. Â My wife...       5            0   \n",
       "2896  Fantastic location, friendly staff and nice ac...       4            0   \n",
       "2897  This boutique hotel is in an excellent locatio...       4            0   \n",
       "2898  My wife and I stayed here July 30th. We arrive...       2            0   \n",
       "2899  Very clean and super helpful staff. Â The rooms...       4            0   \n",
       "2900  My company held an event here and the staff ov...       4            0   \n",
       "2901  This boutique hotel has a very personal approa...       5            0   \n",
       "2902  Wife and I stay here when we visit. Went up on...       5            0   \n",
       "2903  All Westins are definitely nice and so was thi...       4            0   \n",
       "2904  This place was awesome! Well except for the we...       4            0   \n",
       "2905  My friend and I, along with our mothers, staye...       5            0   \n",
       "2906  So Surprising when we arrived, The staff was g...       5            0   \n",
       "2907  Noise, noise, noise! Â Unbelievable! Â Between t...       1            0   \n",
       "\n",
       "      coolCount  funnyCount fake                 hotelID  \n",
       "0             2           6    N  tQfLGoolUMu2J0igcWcoZg  \n",
       "1             4           9    N  tQfLGoolUMu2J0igcWcoZg  \n",
       "2             0           3    N  tQfLGoolUMu2J0igcWcoZg  \n",
       "3             0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "4             0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "5             1           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "6             0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "7             1           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "8             0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "9             0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "10            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "11            1           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "12            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "13            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "14            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "15            0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "16            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "17            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "18            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "19            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "20            1           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "21            0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "22            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "23            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "24            0           1    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "25            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "26            0           0    N  33Xc1Bk_gkSY5xb2doQ7Ng  \n",
       "27            0           0    N  2nnXespKBBNtDQTtrumNFg  \n",
       "28            0           0    N  2nnXespKBBNtDQTtrumNFg  \n",
       "29            0           0    N  2nnXespKBBNtDQTtrumNFg  \n",
       "...         ...         ...  ...                     ...  \n",
       "2878          0           0    N  Gh4Tb2qCBUCOd5r7jqKnGA  \n",
       "2879          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "2880          1           1    N  BISUDalmPulSzHvsO3PhDA  \n",
       "2881          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "2882          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "2883          0           0    N  BISUDalmPulSzHvsO3PhDA  \n",
       "2884          2          11    N  bem-1CTpTNpArpCtThTmFw  \n",
       "2885          2           3    N  bem-1CTpTNpArpCtThTmFw  \n",
       "2886          0           0    Y  bem-1CTpTNpArpCtThTmFw  \n",
       "2887          0           0    Y  bem-1CTpTNpArpCtThTmFw  \n",
       "2888          0           0    Y  g51qDl6fQhgat-kFTrcbug  \n",
       "2889          0           0    Y  g51qDl6fQhgat-kFTrcbug  \n",
       "2890          0           0    Y  BISUDalmPulSzHvsO3PhDA  \n",
       "2891          0           0    Y  Gh4Tb2qCBUCOd5r7jqKnGA  \n",
       "2892          0           0    Y  Gh4Tb2qCBUCOd5r7jqKnGA  \n",
       "2893          0           0    Y  h-U_YfWBK2u_Vqo8AVA4KQ  \n",
       "2894          0           0    Y  h-U_YfWBK2u_Vqo8AVA4KQ  \n",
       "2895          0           0    Y  wLjR0DkA4zxu8iqbUQc0Og  \n",
       "2896          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "2897          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "2898          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "2899          0           0    Y  t5JK4OZzCuetIrJXdHLntg  \n",
       "2900          0           0    Y  S_qmb0Uzm_cNoRjSvZ6y1w  \n",
       "2901          0           0    Y  S_qmb0Uzm_cNoRjSvZ6y1w  \n",
       "2902          0           0    Y  S_qmb0Uzm_cNoRjSvZ6y1w  \n",
       "2903          0           0    Y  LUmAQaRrAleKdXZd8On16Q  \n",
       "2904          0           0    Y  PyG0aSX3pBx0hzoSH20FnA  \n",
       "2905          0           0    Y  PyG0aSX3pBx0hzoSH20FnA  \n",
       "2906          0           0    Y  gCdjyQeE0uRKCh7mVmnZzQ  \n",
       "2907          0           0    Y  rpP9iZsT3NC-Z4pUtQGoiA  \n",
       "\n",
       "[2908 rows x 10 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.set_value(2550, 'reviewContent', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 - Features from paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2908, 10)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maxNumberOfReviews(data):\n",
    "    import datetime\n",
    "    date_old = [datetime.datetime.strptime(x, \"%B %Y\") for x in data[\"yelpJoinDate\"]]\n",
    "    date_new = datetime.datetime.strptime(\"October 2012\", \"%B %Y\") # Data da review mais antiga\n",
    "    data[\"reviewsPerDay\"] = data[\"reviewCount\"] / [(date_new - x).days for x in date_old]\n",
    "    \n",
    "maxNumberOfReviews(reviewers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PPR():\n",
    "    \n",
    "    percentageReviews = []\n",
    "    \n",
    "    for r_id in reviewers_df[\"reviewerID\"]:\n",
    "        train_count = (train_df[\"reviewerID\"] == r_id).sum()\n",
    "        test_count = (test_df[\"reviewerID\"] == r_id).sum()\n",
    "        \n",
    "        \n",
    "        train_count_4 = train_df.loc[(train_df[\"reviewerID\"] == r_id) & (train_df[\"rating\"] > 4)].shape[0]\n",
    "        test_count_4 = test_df.loc[(test_df[\"reviewerID\"] == r_id) & (test_df[\"rating\"] > 4)].shape[0]\n",
    "        \n",
    "        if ((train_count + test_count) == 0):\n",
    "            percentageReviews.append(0.0)\n",
    "        else:\n",
    "            percentageReviews.append((train_count_4 + test_count_4) / (train_count + test_count))\n",
    "\n",
    "    reviewers_df[\"percentageReviews\"] = percentageReviews\n",
    "\n",
    "        \n",
    "    # para cada reviewer fazer get do ID,\n",
    "        # Contar o nÂº de vezes que o ID aparece no train + test com >= 4\n",
    "        # meter no revewer\n",
    "        \n",
    "PPR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from http://stackoverflow.com/questions/15173225/how-to-calculate-cosine-similarity-given-2-sentence-strings-python\n",
    "def cosSim(text1, text2):\n",
    "    import re, math\n",
    "    from collections import Counter\n",
    "\n",
    "    WORD = re.compile(r'\\w+')\n",
    "\n",
    "    def get_cosine(vec1, vec2):\n",
    "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "        if not denominator:\n",
    "            return(0.0)\n",
    "        else:\n",
    "            return(numerator / denominator)\n",
    "\n",
    "    def text_to_vector(text):\n",
    "        words = WORD.findall(text)\n",
    "        return Counter(words)\n",
    "\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "\n",
    "    cosine = get_cosine(vector1, vector2)\n",
    "    return(cosine)\n",
    "    \n",
    "def similarityUsers():\n",
    "    import itertools as it\n",
    "    \n",
    "    mean_distance = []\n",
    "    \n",
    "    for r_id in reviewers_df[\"reviewerID\"]:\n",
    "        texts_train = train_df[train_df[\"reviewerID\"] == r_id][\"reviewContent\"]\n",
    "        texts_test = test_df[test_df[\"reviewerID\"] == r_id][\"reviewContent\"]\n",
    "        texts = np.append(texts_train, texts_test)\n",
    "        \n",
    "        #print(\"tamanho do texto %d\" % len(texts))\n",
    "        if (len(texts) == 0 or len(texts) == 1):\n",
    "            mean_distance.append(0.0)\n",
    "        else:\n",
    "            # Get all combinations of reviews from an user:\n",
    "            combs = list(it.combinations(texts,2))\n",
    "            #print(len(texts))\n",
    "            #print(len(combs))\n",
    "            dist_list = [cosSim(comb[0], comb[1]) for comb in list(it.combinations(texts,2))]\n",
    "            #print(dist_list)\n",
    "\n",
    "            # Calculate the mean distance for the user's reviews\n",
    "            mean_distance.append(np.mean(np.array(dist_list)))\n",
    "        \n",
    "    reviewers_df[\"reviewSimilarity\"] = mean_distance \n",
    "    # percorrer os users\n",
    "        # buscar todos os textos do user (indices)\n",
    "            # ver a funÃ§Ã£o das combinaÃ§Ãµes e pedir a cosSim para todos, no fim fazer a media\n",
    "similarityUsers()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df_joined = train_df.add_suffix(\"_review\").join(reviewers_df.add_suffix(\"_user\")).join(hotels_df.add_suffix(\"_hotel\")).drop([\n",
    "        \"reviewerID_review\",\n",
    "        \"hotelID_review\"\n",
    "    ], axis=1)\n",
    "\n",
    "test_df_joined = test_df.add_suffix(\"_review\").join(reviewers_df.add_suffix(\"_user\")).join(hotels_df.add_suffix(\"_hotel\")).drop([\n",
    "        \"reviewerID_review\",\n",
    "        \"hotelID_review\"\n",
    "    ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date_review', 'reviewID_review', 'reviewContent_review',\n",
       "       'rating_review', 'usefulCount_review', 'coolCount_review',\n",
       "       'funnyCount_review', 'fake_review', 'reviewerID_user', 'name_user',\n",
       "       'location_user', 'yelpJoinDate_user', 'friendCount_user',\n",
       "       'reviewCount_user', 'firstCount_user', 'usefulCount_user',\n",
       "       'coolCount_user', 'funnyCount_user', 'complimentCount_user',\n",
       "       'tipCount_user', 'fanCount_user', 'reviewsPerDay_user',\n",
       "       'percentageReviews_user', 'reviewSimilarity_user', 'hotelID_hotel',\n",
       "       'name_hotel', 'location_hotel', 'reviewCount_hotel', 'rating_hotel',\n",
       "       'categories_hotel', 'address_hotel', 'AcceptsCreditCards_hotel',\n",
       "       'PriceRange_hotel', 'WiFi_hotel', 'webSite_hotel', 'phoneNumber_hotel',\n",
       "       'filReviewCount_hotel'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_joined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df_joined.to_csv(\"train_joined.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 - Text Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lengthReview(data):\n",
    "    data['text_length'] = data['reviewContent_review'].str.len() \n",
    "    data['word_count'] = data['reviewContent_review'].str.count(' ')\n",
    "    \n",
    "lengthReview(train_df_joined)\n",
    "lengthReview(test_df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# bag of words based in the words referenced in the paper\n",
    "def bagOfWords(data): \n",
    "    personal_pronouns = ['i', 'we', 'me', 'us', 'my', 'mine', 'our', 'ours']\n",
    "    associated_actions = ['went', 'feel']\n",
    "    targets = ['area', 'options', 'price', 'stay']\n",
    "    emotion_words = ['nice', 'deal', 'comfort', 'helpful']\n",
    "\n",
    "    import collections, re\n",
    "\n",
    "    # put in lower case\n",
    "    data['reviewContent_review'] = data['reviewContent_review'].str.lower()\n",
    "    \n",
    "    # Replace characters\n",
    "    data['reviewContent_review'] = data['reviewContent_review'].str.replace('\\xa0',' ')\n",
    "    \n",
    "    texts = list(data['reviewContent_review'])\n",
    "    bagsofwords = [collections.Counter(re.findall(r'\\w+', txt)) \n",
    "                   for txt in texts]\n",
    "\n",
    "    def count_words(data, word_list, attribute_name):\n",
    "        data[attribute_name] = np.nan\n",
    "        for i in range(0, data.shape[0]):\n",
    "            num = 0\n",
    "            for word in word_list:\n",
    "                num += bagsofwords[i][word]\n",
    "                \n",
    "            if (data[\"word_count\"][i] == 0):\n",
    "                data.set_value(i, attribute_name, 0.0)\n",
    "            else:\n",
    "                data.set_value(i, attribute_name, num/data[\"word_count\"][i])\n",
    "\n",
    "    count_words(data, personal_pronouns, 'count_pronouns')\n",
    "    count_words(data, associated_actions, 'count_associated_actions')\n",
    "    count_words(data, targets, 'count_targets')\n",
    "    count_words(data, emotion_words, 'count_emotion_words')\n",
    "    \n",
    "bagOfWords(train_df_joined)\n",
    "bagOfWords(test_df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2. - Other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deviationFromHotelRating(data):\n",
    "    data[\"deviatonReviewHotel\"] = data[\"rating_review\"] - data[\"rating_hotel\"]\n",
    "\n",
    "deviationFromHotelRating(train_df_joined)\n",
    "deviationFromHotelRating(test_df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def countsUserReview(data):\n",
    "    data[\"popularity_review\"] = data[\"usefulCount_review\"] + data[\"coolCount_review\"] + data[\"funnyCount_review\"]\n",
    "        \n",
    "    data[\"popularity_user\"] = data[\"usefulCount_user\"] + data[\"coolCount_user\"] + data[\"funnyCount_user\"] + data[\"complimentCount_user\"] + data[\"fanCount_user\"]\n",
    "\n",
    "countsUserReview(train_df_joined)\n",
    "countsUserReview(test_df_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_df_joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def undersampling(data):\n",
    "    ids_to_sample = data[data[\"fake_review\"] == \"N\"].index.values\n",
    "    sample_size = data[data[\"fake_review\"] == \"Y\"].shape[0]\n",
    "    anomalies = data[data[\"fake_review\"] == \"Y\"].index.values\n",
    "    \n",
    "    final_ids = np.append(np.random.choice(ids_to_sample, size = sample_size), anomalies)\n",
    "    #print(ids_to_sample)\n",
    "    #data[fake_review]\n",
    "    return data.iloc[final_ids]\n",
    "    \n",
    "train_df_joined = undersampling(train_df_joined)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Define global variables for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      text_length  popularity_review\n",
      "1947         1367                  9\n",
      "970          2565                  1\n",
      "2207          435                  0\n",
      "588           590                  0\n",
      "2158         1237                 22\n",
      "2811          351                  0\n",
      "276           988                  0\n",
      "2151         1200                  1\n",
      "244          1467                  3\n",
      "2807          397                  0\n",
      "1052          148                  1\n",
      "2791          580                  4\n",
      "1449         1525                  1\n",
      "434          1103                  0\n",
      "1270         3021                  2\n",
      "751          2282                  3\n",
      "2720          344                  5\n",
      "1258          985                  4\n",
      "1401         1585                  3\n",
      "449           291                  1\n",
      "1402         2482                  3\n",
      "657          1083                  1\n",
      "25           1342                  2\n",
      "35            509                  1\n",
      "793          1493                  0\n",
      "1122          448                  0\n",
      "1474          266                  0\n",
      "746          2265                  6\n",
      "136           588                  7\n",
      "2101         2992                  0\n",
      "...           ...                ...\n",
      "2703         2283                  0\n",
      "2704          418                  0\n",
      "2705          301                  0\n",
      "2706          347                  0\n",
      "2707         1618                  0\n",
      "2708         1207                  0\n",
      "2709           72                  0\n",
      "2710          201                  0\n",
      "2886         2049                  0\n",
      "2887           70                  0\n",
      "2888          170                  0\n",
      "2889          345                  0\n",
      "2890          227                  0\n",
      "2891          422                  0\n",
      "2892          740                  0\n",
      "2893          870                  0\n",
      "2894          777                  0\n",
      "2895          559                  0\n",
      "2896          619                  0\n",
      "2897          578                  0\n",
      "2898          670                  0\n",
      "2899          655                  0\n",
      "2900         1178                  0\n",
      "2901         1343                  0\n",
      "2902          652                  0\n",
      "2903          228                  0\n",
      "2904          471                  0\n",
      "2905          534                  0\n",
      "2906          605                  0\n",
      "2907          139                  0\n",
      "\n",
      "[784 rows x 2 columns]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "def encodeVariables():\n",
    "    \n",
    "    target_variable = \"fake_review\" # target variable\n",
    "    \n",
    "    dropVariableList = [\n",
    "        \n",
    "        ###all list of variables, then choose the ones to keep\n",
    "        \n",
    "        #REVIEW FEATURES\n",
    "        \n",
    "        'date_review', #not useful\n",
    "        'reviewID_review', #not useful\n",
    "        'reviewContent_review', #not useful after having the features\n",
    "        'rating_review', \n",
    "        'usefulCount_review', #correlated 1\n",
    "        'coolCount_review',  #correlated 1\n",
    "        'funnyCount_review', #correlated 1\n",
    "        #'popularity_review', \n",
    "        \n",
    "        #USER FEATURES\n",
    "        \n",
    "        'reviewerID_user', #not useful\n",
    "        'name_user', #not useful\n",
    "        'location_user', #not useful\n",
    "        'yelpJoinDate_user', #not useful\n",
    "        'friendCount_user', #mean proves it distinguish among the groups, #based on feature selection\n",
    "        #'reviewCount_user', #mean proves it distinguish among the groups\n",
    "        'firstCount_user', # very strange patern in the mean values\n",
    "        'usefulCount_user', # correlated 2\n",
    "        'coolCount_user', # correlated 2\n",
    "        'funnyCount_user', # correlated 2\n",
    "        'complimentCount_user', # correlated 2\n",
    "        'tipCount_user', # very strange patern in the mean values\n",
    "        'fanCount_user', # correlated 2\n",
    "        #'reviewsPerDay_user', \n",
    "        'percentageReviews_user', #based on feature selection\n",
    "        'reviewSimilarity_user', #based on feature selection\n",
    "        'popularity_user', #based on feature selection\n",
    "        \n",
    "        #HOTEL FEATURES\n",
    "        \n",
    "        'hotelID_hotel', #not useful\n",
    "        'name_hotel',  #not useful\n",
    "        'location_hotel', #not useful\n",
    "        'reviewCount_hotel',# correlated 3\n",
    "        'rating_hotel', # correlated 4\n",
    "        'categories_hotel', #not useful\n",
    "        'address_hotel', #not useful\n",
    "        'AcceptsCreditCards_hotel', #not useful\n",
    "        'PriceRange_hotel', #not useful \n",
    "        'WiFi_hotel', #not useful\n",
    "        'webSite_hotel', #not useful\n",
    "        'phoneNumber_hotel', #not useful\n",
    "        'filReviewCount_hotel', #based on feature selection\n",
    "        #'deviatonReviewHotel',\n",
    "        \n",
    "\n",
    "        \n",
    "        #TEXT MINING FEATURES\n",
    "        #'text_length'\n",
    "        'word_count', #based on feature selection\n",
    "        'count_pronouns', #based on feature selection\n",
    "        'count_associated_actions',#based on feature selection\n",
    "        'count_targets', #based on feature selection\n",
    "        'count_emotion_words'#based on feature selection\n",
    "        \n",
    "        ]\n",
    " \n",
    "    # Drop variables in the X\n",
    "    X_train = train_df_joined.drop(dropVariableList, axis=1)\n",
    "    X_train = X_train.drop(target_variable, axis=1)\n",
    "    X_test = test_df_joined.drop(dropVariableList, axis=1)\n",
    "    X_test = X_test.drop(\"id_review\", axis=1)\n",
    "    \n",
    "    # Drop variables in the Y\n",
    "    Y_train = train_df_joined[\"fake_review\"]\n",
    "\n",
    "    # Transform categorical variables for X_train:\n",
    "    categoricalVariableList = [\n",
    "        #\"AcceptsCreditCards_hotel\",\n",
    "        #\"PriceRange_hotel\",\n",
    "        #\"WiFi_hotel\"\n",
    "    ]\n",
    "    \n",
    "    for var_name in categoricalVariableList:\n",
    "        encoder = skpre.LabelEncoder().fit(X_train[var_name])\n",
    "        X_train[var_name] = encoder.transform(X_train[var_name])\n",
    "        X_test[var_name] = encoder.transform(X_test[var_name])\n",
    "\n",
    "    # Transform categorical variables for Y_train\n",
    "    Y_train = skpre.LabelEncoder().fit_transform(Y_train)\n",
    "\n",
    "    print(X_train)\n",
    "    #print(X_test)\n",
    "    print(Y_train)\n",
    "    \n",
    "    return X_train, Y_train, X_test\n",
    "    \n",
    "X_train, Y_train, X_test = encodeVariables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>friendCount_user</th>\n",
       "      <th>reviewCount_user</th>\n",
       "      <th>reviewsPerDay_user</th>\n",
       "      <th>percentageReviews_user</th>\n",
       "      <th>reviewSimilarity_user</th>\n",
       "      <th>filReviewCount_hotel</th>\n",
       "      <th>text_length</th>\n",
       "      <th>word_count</th>\n",
       "      <th>count_pronouns</th>\n",
       "      <th>count_associated_actions</th>\n",
       "      <th>count_targets</th>\n",
       "      <th>count_emotion_words</th>\n",
       "      <th>deviatonReviewHotel</th>\n",
       "      <th>popularity_review</th>\n",
       "      <th>popularity_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1608</th>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>0.013680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2383</td>\n",
       "      <td>423</td>\n",
       "      <td>0.070922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.018248</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1750</td>\n",
       "      <td>314</td>\n",
       "      <td>0.041401</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2884</th>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>0.065693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>721</td>\n",
       "      <td>130</td>\n",
       "      <td>0.023077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099</th>\n",
       "      <td>49</td>\n",
       "      <td>383</td>\n",
       "      <td>0.433258</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>522</td>\n",
       "      <td>93</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2778</th>\n",
       "      <td>4531</td>\n",
       "      <td>939</td>\n",
       "      <td>0.685902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>648</td>\n",
       "      <td>119</td>\n",
       "      <td>0.067227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>33773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>674</th>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.155738</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>455</td>\n",
       "      <td>77</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.012987</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>46</td>\n",
       "      <td>88</td>\n",
       "      <td>0.068804</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>901</td>\n",
       "      <td>166</td>\n",
       "      <td>0.036145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018072</td>\n",
       "      <td>0.024096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1071</th>\n",
       "      <td>17</td>\n",
       "      <td>155</td>\n",
       "      <td>0.164021</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1354</td>\n",
       "      <td>250</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2822</th>\n",
       "      <td>44</td>\n",
       "      <td>153</td>\n",
       "      <td>0.066988</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1101</td>\n",
       "      <td>203</td>\n",
       "      <td>0.064039</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.004926</td>\n",
       "      <td>0.009852</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.006823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>318</td>\n",
       "      <td>56</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1219</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>41</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "      <td>34</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.005186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>91</td>\n",
       "      <td>0.043956</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021978</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>2323</td>\n",
       "      <td>1552</td>\n",
       "      <td>1.184733</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1263</td>\n",
       "      <td>227</td>\n",
       "      <td>0.070485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>82634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>498</td>\n",
       "      <td>691</td>\n",
       "      <td>0.348990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "      <td>957</td>\n",
       "      <td>172</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005814</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>9172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2777</th>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0.065663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>759</td>\n",
       "      <td>142</td>\n",
       "      <td>0.105634</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>0.061753</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>740</td>\n",
       "      <td>134</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0.032755</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>36</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1349</th>\n",
       "      <td>301</td>\n",
       "      <td>575</td>\n",
       "      <td>0.222265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>800</td>\n",
       "      <td>134</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.014925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1112</th>\n",
       "      <td>23</td>\n",
       "      <td>111</td>\n",
       "      <td>0.456790</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>505</td>\n",
       "      <td>87</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1809</td>\n",
       "      <td>331</td>\n",
       "      <td>0.063444</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006042</td>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>11</td>\n",
       "      <td>29</td>\n",
       "      <td>0.047697</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1749</td>\n",
       "      <td>315</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015873</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>472</td>\n",
       "      <td>933</td>\n",
       "      <td>0.471212</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>592</td>\n",
       "      <td>100</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>0.021715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>181</td>\n",
       "      <td>30</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>0.041854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>700</td>\n",
       "      <td>111</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001927</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1337</td>\n",
       "      <td>237</td>\n",
       "      <td>0.059072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0.043103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1088</td>\n",
       "      <td>187</td>\n",
       "      <td>0.037433</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1585</th>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0.024105</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1328</td>\n",
       "      <td>243</td>\n",
       "      <td>0.049383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2023</td>\n",
       "      <td>401</td>\n",
       "      <td>0.074813</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.032609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1727</td>\n",
       "      <td>335</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.008955</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>49</td>\n",
       "      <td>269</td>\n",
       "      <td>0.149777</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2283</td>\n",
       "      <td>410</td>\n",
       "      <td>0.029268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>744</td>\n",
       "      <td>563</td>\n",
       "      <td>0.369908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>418</td>\n",
       "      <td>78</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>11</td>\n",
       "      <td>137</td>\n",
       "      <td>0.072564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>51</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0.013611</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>347</td>\n",
       "      <td>65</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.030769</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1618</td>\n",
       "      <td>292</td>\n",
       "      <td>0.047945</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010274</td>\n",
       "      <td>0.017123</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>63</td>\n",
       "      <td>170</td>\n",
       "      <td>0.159624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1207</td>\n",
       "      <td>245</td>\n",
       "      <td>0.057143</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>15</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>32</td>\n",
       "      <td>124</td>\n",
       "      <td>0.088571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>201</td>\n",
       "      <td>40</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>199</td>\n",
       "      <td>290</td>\n",
       "      <td>0.198494</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2049</td>\n",
       "      <td>379</td>\n",
       "      <td>0.063325</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.005277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>70</td>\n",
       "      <td>149</td>\n",
       "      <td>0.065236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0.019547</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>170</td>\n",
       "      <td>30</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>36</td>\n",
       "      <td>36</td>\n",
       "      <td>0.032847</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>345</td>\n",
       "      <td>58</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>0.059468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>44</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>31</td>\n",
       "      <td>129</td>\n",
       "      <td>0.052333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "      <td>78</td>\n",
       "      <td>0.089744</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>0.015714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>740</td>\n",
       "      <td>143</td>\n",
       "      <td>0.055944</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>188</td>\n",
       "      <td>626</td>\n",
       "      <td>0.236316</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>870</td>\n",
       "      <td>174</td>\n",
       "      <td>0.063218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>51</td>\n",
       "      <td>385</td>\n",
       "      <td>0.200625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>777</td>\n",
       "      <td>137</td>\n",
       "      <td>0.080292</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>19</td>\n",
       "      <td>79</td>\n",
       "      <td>0.064860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>559</td>\n",
       "      <td>102</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "      <td>116</td>\n",
       "      <td>0.094828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>578</td>\n",
       "      <td>97</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>1155</td>\n",
       "      <td>337</td>\n",
       "      <td>0.299290</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>670</td>\n",
       "      <td>135</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>39299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>22</td>\n",
       "      <td>41</td>\n",
       "      <td>0.039614</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>655</td>\n",
       "      <td>115</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026087</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>0.115132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1178</td>\n",
       "      <td>208</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004808</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>29</td>\n",
       "      <td>107</td>\n",
       "      <td>0.054900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1343</td>\n",
       "      <td>237</td>\n",
       "      <td>0.080169</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.004219</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>0.073770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>652</td>\n",
       "      <td>117</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0.072368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>228</td>\n",
       "      <td>44</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>471</td>\n",
       "      <td>79</td>\n",
       "      <td>0.050633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>0.039859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>534</td>\n",
       "      <td>93</td>\n",
       "      <td>0.107527</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>19</td>\n",
       "      <td>50</td>\n",
       "      <td>0.024876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>605</td>\n",
       "      <td>110</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>0.030116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows Ã 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      friendCount_user  reviewCount_user  reviewsPerDay_user  \\\n",
       "1608                36                10            0.013680   \n",
       "2717                 0                 5            0.018248   \n",
       "2884                10                18            0.065693   \n",
       "1099                49               383            0.433258   \n",
       "2778              4531               939            0.685902   \n",
       "674                  0                76            0.155738   \n",
       "433                 46                88            0.068804   \n",
       "1071                17               155            0.164021   \n",
       "2822                44               153            0.066988   \n",
       "124                  4                16            0.006823   \n",
       "1219                 0                 7            0.007407   \n",
       "1126                 0                 5            0.009634   \n",
       "938                  0                 6            0.005186   \n",
       "875               2323              1552            1.184733   \n",
       "51                 498               691            0.348990   \n",
       "2777                 0                48            0.065663   \n",
       "892                 10                62            0.061753   \n",
       "255                  0                17            0.032755   \n",
       "1349               301               575            0.222265   \n",
       "1112                23               111            0.456790   \n",
       "1326                 0                 5            0.009107   \n",
       "1375                11                29            0.047697   \n",
       "2169               472               933            0.471212   \n",
       "31                   6                39            0.021715   \n",
       "2868                 7                65            0.041854   \n",
       "1588                 0                 1            0.001927   \n",
       "132                  8                25            0.043103   \n",
       "1585                 5                33            0.024105   \n",
       "1831                 2                12            0.006356   \n",
       "2047                 0                 3            0.032609   \n",
       "...                ...               ...                 ...   \n",
       "2703                49               269            0.149777   \n",
       "2704               744               563            0.369908   \n",
       "2705                11               137            0.072564   \n",
       "2706                 5                17            0.013611   \n",
       "2707                34                14            0.017677   \n",
       "2708                63               170            0.159624   \n",
       "2709                 0                 5            0.166667   \n",
       "2710                32               124            0.088571   \n",
       "2886               199               290            0.198494   \n",
       "2887                70               149            0.065236   \n",
       "2888                 6                25            0.019547   \n",
       "2889                36                36            0.032847   \n",
       "2890                 3                38            0.059468   \n",
       "2891                31               129            0.052333   \n",
       "2892                35                22            0.015714   \n",
       "2893               188               626            0.236316   \n",
       "2894                51               385            0.200625   \n",
       "2895                19                79            0.064860   \n",
       "2896                 1                 1            0.001724   \n",
       "2897                 0                 2            0.002345   \n",
       "2898              1155               337            0.299290   \n",
       "2899                22                41            0.039614   \n",
       "2900                36                70            0.115132   \n",
       "2901                29               107            0.054900   \n",
       "2902                 0                36            0.073770   \n",
       "2903                 0                44            0.072368   \n",
       "2904                 0                 5            0.010246   \n",
       "2905                 9                34            0.039859   \n",
       "2906                19                50            0.024876   \n",
       "2907                88                44            0.030116   \n",
       "\n",
       "      percentageReviews_user  reviewSimilarity_user  filReviewCount_hotel  \\\n",
       "1608                     0.0                    0.0                     0   \n",
       "2717                     1.0                    0.0                     0   \n",
       "2884                     1.0                    0.0                     0   \n",
       "1099                     0.0                    0.0                     0   \n",
       "2778                     0.0                    0.0                     0   \n",
       "674                      0.0                    0.0                     0   \n",
       "433                      0.0                    0.0                     0   \n",
       "1071                     1.0                    0.0                     0   \n",
       "2822                     0.0                    0.0                     0   \n",
       "124                      0.0                    0.0                     0   \n",
       "1219                     0.0                    0.0                     0   \n",
       "1126                     0.0                    0.0                     0   \n",
       "938                      0.0                    0.0                     0   \n",
       "875                      0.0                    0.0                     0   \n",
       "51                       0.0                    0.0                    26   \n",
       "2777                     0.0                    0.0                     0   \n",
       "892                      0.0                    0.0                     0   \n",
       "255                      0.0                    0.0                     0   \n",
       "1349                     0.0                    0.0                     0   \n",
       "1112                     0.0                    0.0                     0   \n",
       "1326                     0.0                    0.0                     0   \n",
       "1375                     0.0                    0.0                     0   \n",
       "2169                     1.0                    0.0                     0   \n",
       "31                       0.0                    0.0                    10   \n",
       "2868                     0.0                    0.0                     0   \n",
       "1588                     1.0                    0.0                     0   \n",
       "132                      0.0                    0.0                     0   \n",
       "1585                     1.0                    0.0                     0   \n",
       "1831                     1.0                    0.0                     0   \n",
       "2047                     0.0                    0.0                     0   \n",
       "...                      ...                    ...                   ...   \n",
       "2703                     0.0                    0.0                     0   \n",
       "2704                     0.0                    0.0                     0   \n",
       "2705                     0.0                    0.0                     0   \n",
       "2706                     0.0                    0.0                     0   \n",
       "2707                     0.0                    0.0                     0   \n",
       "2708                     0.0                    0.0                     0   \n",
       "2709                     1.0                    0.0                     0   \n",
       "2710                     0.0                    0.0                     0   \n",
       "2886                     0.0                    0.0                     0   \n",
       "2887                     0.0                    0.0                     0   \n",
       "2888                     0.0                    0.0                     0   \n",
       "2889                     1.0                    0.0                     0   \n",
       "2890                     0.0                    0.0                     0   \n",
       "2891                     0.0                    0.0                     0   \n",
       "2892                     0.0                    0.0                     0   \n",
       "2893                     0.0                    0.0                     0   \n",
       "2894                     0.0                    0.0                     0   \n",
       "2895                     0.0                    0.0                     0   \n",
       "2896                     0.0                    0.0                     0   \n",
       "2897                     0.0                    0.0                     0   \n",
       "2898                     0.0                    0.0                     0   \n",
       "2899                     0.0                    0.0                     0   \n",
       "2900                     0.0                    0.0                     0   \n",
       "2901                     0.0                    0.0                     0   \n",
       "2902                     0.0                    0.0                     0   \n",
       "2903                     0.0                    0.0                     0   \n",
       "2904                     1.0                    0.0                     0   \n",
       "2905                     0.0                    0.0                     0   \n",
       "2906                     0.0                    0.0                     0   \n",
       "2907                     1.0                    0.0                     0   \n",
       "\n",
       "      text_length  word_count  count_pronouns  count_associated_actions  \\\n",
       "1608         2383         423        0.070922                  0.000000   \n",
       "2717         1750         314        0.041401                  0.000000   \n",
       "2884          721         130        0.023077                  0.000000   \n",
       "1099          522          93        0.000000                  0.000000   \n",
       "2778          648         119        0.067227                  0.000000   \n",
       "674           455          77        0.012987                  0.000000   \n",
       "433           901         166        0.036145                  0.000000   \n",
       "1071         1354         250        0.060000                  0.000000   \n",
       "2822         1101         203        0.064039                  0.004926   \n",
       "124           318          56        0.071429                  0.000000   \n",
       "1219          232          41        0.048780                  0.000000   \n",
       "1126          205          34        0.088235                  0.000000   \n",
       "938           469          91        0.043956                  0.000000   \n",
       "875          1263         227        0.070485                  0.000000   \n",
       "51            957         172        0.069767                  0.000000   \n",
       "2777          759         142        0.105634                  0.000000   \n",
       "892           740         134        0.014925                  0.014925   \n",
       "255           180          36        0.055556                  0.000000   \n",
       "1349          800         134        0.014925                  0.000000   \n",
       "1112          505          87        0.000000                  0.000000   \n",
       "1326         1809         331        0.063444                  0.000000   \n",
       "1375         1749         315        0.028571                  0.003175   \n",
       "2169          592         100        0.050000                  0.000000   \n",
       "31            181          30        0.066667                  0.000000   \n",
       "2868          700         111        0.027027                  0.000000   \n",
       "1588         1337         237        0.059072                  0.000000   \n",
       "132          1088         187        0.037433                  0.000000   \n",
       "1585         1328         243        0.049383                  0.000000   \n",
       "1831         2023         401        0.074813                  0.004988   \n",
       "2047         1727         335        0.056716                  0.002985   \n",
       "...           ...         ...             ...                       ...   \n",
       "2703         2283         410        0.029268                  0.000000   \n",
       "2704          418          78        0.038462                  0.000000   \n",
       "2705          301          51        0.019608                  0.000000   \n",
       "2706          347          65        0.123077                  0.000000   \n",
       "2707         1618         292        0.047945                  0.000000   \n",
       "2708         1207         245        0.057143                  0.004082   \n",
       "2709           72          15        0.066667                  0.000000   \n",
       "2710          201          40        0.000000                  0.000000   \n",
       "2886         2049         379        0.063325                  0.002639   \n",
       "2887           70          11        0.000000                  0.000000   \n",
       "2888          170          30        0.033333                  0.000000   \n",
       "2889          345          58        0.034483                  0.000000   \n",
       "2890          227          44        0.068182                  0.000000   \n",
       "2891          422          78        0.089744                  0.000000   \n",
       "2892          740         143        0.055944                  0.013986   \n",
       "2893          870         174        0.063218                  0.000000   \n",
       "2894          777         137        0.080292                  0.000000   \n",
       "2895          559         102        0.098039                  0.000000   \n",
       "2896          619         116        0.094828                  0.000000   \n",
       "2897          578          97        0.030928                  0.000000   \n",
       "2898          670         135        0.103704                  0.000000   \n",
       "2899          655         115        0.043478                  0.000000   \n",
       "2900         1178         208        0.033654                  0.000000   \n",
       "2901         1343         237        0.080169                  0.008439   \n",
       "2902          652         117        0.076923                  0.008547   \n",
       "2903          228          44        0.045455                  0.000000   \n",
       "2904          471          79        0.050633                  0.000000   \n",
       "2905          534          93        0.107527                  0.010753   \n",
       "2906          605         110        0.054545                  0.000000   \n",
       "2907          139          20        0.000000                  0.000000   \n",
       "\n",
       "      count_targets  count_emotion_words  deviatonReviewHotel  \\\n",
       "1608       0.004728             0.000000                 -1.5   \n",
       "2717       0.000000             0.003185                 -2.0   \n",
       "2884       0.000000             0.000000                 -2.5   \n",
       "1099       0.000000             0.010753                  1.0   \n",
       "2778       0.000000             0.016807                  0.5   \n",
       "674        0.012987             0.012987                  0.5   \n",
       "433        0.018072             0.024096                  0.0   \n",
       "1071       0.000000             0.004000                  0.5   \n",
       "2822       0.004926             0.009852                  1.0   \n",
       "124        0.017857             0.000000                  1.5   \n",
       "1219       0.000000             0.000000                  1.0   \n",
       "1126       0.000000             0.000000                  3.0   \n",
       "938        0.021978             0.010989                  1.5   \n",
       "875        0.000000             0.004405                 -3.0   \n",
       "51         0.005814             0.011628                  0.0   \n",
       "2777       0.007042             0.000000                 -2.0   \n",
       "892        0.014925             0.000000                 -0.5   \n",
       "255        0.055556             0.000000                  0.5   \n",
       "1349       0.007463             0.014925                  0.0   \n",
       "1112       0.011494             0.000000                  0.5   \n",
       "1326       0.006042             0.009063                  0.5   \n",
       "1375       0.000000             0.015873                  2.0   \n",
       "2169       0.020000             0.010000                  0.0   \n",
       "31         0.000000             0.000000                 -1.5   \n",
       "2868       0.009009             0.018018                  1.5   \n",
       "1588       0.008439             0.000000                 -1.5   \n",
       "132        0.000000             0.000000                  1.5   \n",
       "1585       0.012346             0.008230                 -1.0   \n",
       "1831       0.004988             0.004988                 -0.5   \n",
       "2047       0.008955             0.002985                 -3.0   \n",
       "...             ...                  ...                  ...   \n",
       "2703       0.004878             0.002439                 -2.5   \n",
       "2704       0.000000             0.000000                 -1.0   \n",
       "2705       0.039216             0.000000                 -3.0   \n",
       "2706       0.015385             0.030769                 -1.0   \n",
       "2707       0.010274             0.017123                 -0.5   \n",
       "2708       0.012245             0.000000                 -0.5   \n",
       "2709       0.000000             0.000000                  1.0   \n",
       "2710       0.000000             0.075000                 -3.0   \n",
       "2886       0.005277             0.000000                 -2.0   \n",
       "2887       0.000000             0.000000                  3.5   \n",
       "2888       0.000000             0.000000                 -1.5   \n",
       "2889       0.000000             0.000000                 -2.5   \n",
       "2890       0.045455             0.000000                 -1.0   \n",
       "2891       0.038462             0.012821                  1.0   \n",
       "2892       0.000000             0.000000                 -1.5   \n",
       "2893       0.022989             0.000000                 -3.0   \n",
       "2894       0.007299             0.000000                 -2.0   \n",
       "2895       0.000000             0.000000                  1.0   \n",
       "2896       0.008621             0.008621                  0.0   \n",
       "2897       0.010309             0.000000                  1.0   \n",
       "2898       0.000000             0.014815                 -1.0   \n",
       "2899       0.000000             0.026087                  1.5   \n",
       "2900       0.004808             0.009615                  1.5   \n",
       "2901       0.008439             0.004219                  1.5   \n",
       "2902       0.008547             0.008547                  1.5   \n",
       "2903       0.000000             0.022727                  1.0   \n",
       "2904       0.000000             0.000000                  2.0   \n",
       "2905       0.021505             0.000000                  1.5   \n",
       "2906       0.009091             0.000000                  2.0   \n",
       "2907       0.000000             0.000000                 -2.5   \n",
       "\n",
       "      popularity_review  popularity_user  \n",
       "1608                  0               17  \n",
       "2717                  5                2  \n",
       "2884                 17               18  \n",
       "1099                  2              671  \n",
       "2778                  4            33773  \n",
       "674                   0               56  \n",
       "433                   0              175  \n",
       "1071                  0              445  \n",
       "2822                  7              525  \n",
       "124                   2               53  \n",
       "1219                  0               10  \n",
       "1126                  0                3  \n",
       "938                   0                4  \n",
       "875                   7            82634  \n",
       "51                   37             9172  \n",
       "2777                  0               42  \n",
       "892                   0              100  \n",
       "255                   0               65  \n",
       "1349                  2             2721  \n",
       "1112                  0              259  \n",
       "1326                  3                0  \n",
       "1375                  2               31  \n",
       "2169                  0            15619  \n",
       "31                    0               38  \n",
       "2868                  2              175  \n",
       "1588                 16                1  \n",
       "132                   1               58  \n",
       "1585                  3               73  \n",
       "1831                  0               10  \n",
       "2047                  3                2  \n",
       "...                 ...              ...  \n",
       "2703                  0             2091  \n",
       "2704                  0             6162  \n",
       "2705                  0              200  \n",
       "2706                  0               28  \n",
       "2707                  0               12  \n",
       "2708                  0              376  \n",
       "2709                  0                0  \n",
       "2710                  0              244  \n",
       "2886                  0             3001  \n",
       "2887                  0              131  \n",
       "2888                  0               39  \n",
       "2889                  0               81  \n",
       "2890                  0               15  \n",
       "2891                  0              327  \n",
       "2892                  0               14  \n",
       "2893                  0              918  \n",
       "2894                  0              850  \n",
       "2895                  0               63  \n",
       "2896                  0                3  \n",
       "2897                  0                1  \n",
       "2898                  0            39299  \n",
       "2899                  0              206  \n",
       "2900                  0              135  \n",
       "2901                  0              379  \n",
       "2902                  0               10  \n",
       "2903                  0               44  \n",
       "2904                  0                2  \n",
       "2905                  0               53  \n",
       "2906                  0              347  \n",
       "2907                  0              210  \n",
       "\n",
       "[784 rows x 15 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text_length', 'popularity_review'], dtype='object')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def featureNormalization():\n",
    "    global X_train\n",
    "    global X_test\n",
    "    \n",
    "    scaler = skpre.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "#featureNormalization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>popularity_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>1367</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>2565</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>435</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>590</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>1237</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2811</th>\n",
       "      <td>351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>988</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2151</th>\n",
       "      <td>1200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1467</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>148</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2791</th>\n",
       "      <td>580</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449</th>\n",
       "      <td>1525</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>3021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>2282</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2720</th>\n",
       "      <td>344</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>985</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>1585</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>291</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>2482</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>657</th>\n",
       "      <td>1083</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1342</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>1493</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1122</th>\n",
       "      <td>448</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1474</th>\n",
       "      <td>266</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>2265</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>588</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>2992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>2283</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>347</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2707</th>\n",
       "      <td>1618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2708</th>\n",
       "      <td>1207</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2709</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2710</th>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2886</th>\n",
       "      <td>2049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2888</th>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2889</th>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2890</th>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>422</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>740</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>870</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>777</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2895</th>\n",
       "      <td>559</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2896</th>\n",
       "      <td>619</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>578</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>670</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>655</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>1178</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>1343</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>652</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2903</th>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2904</th>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2906</th>\n",
       "      <td>605</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2907</th>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>784 rows Ã 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      text_length  popularity_review\n",
       "1947         1367                  9\n",
       "970          2565                  1\n",
       "2207          435                  0\n",
       "588           590                  0\n",
       "2158         1237                 22\n",
       "2811          351                  0\n",
       "276           988                  0\n",
       "2151         1200                  1\n",
       "244          1467                  3\n",
       "2807          397                  0\n",
       "1052          148                  1\n",
       "2791          580                  4\n",
       "1449         1525                  1\n",
       "434          1103                  0\n",
       "1270         3021                  2\n",
       "751          2282                  3\n",
       "2720          344                  5\n",
       "1258          985                  4\n",
       "1401         1585                  3\n",
       "449           291                  1\n",
       "1402         2482                  3\n",
       "657          1083                  1\n",
       "25           1342                  2\n",
       "35            509                  1\n",
       "793          1493                  0\n",
       "1122          448                  0\n",
       "1474          266                  0\n",
       "746          2265                  6\n",
       "136           588                  7\n",
       "2101         2992                  0\n",
       "...           ...                ...\n",
       "2703         2283                  0\n",
       "2704          418                  0\n",
       "2705          301                  0\n",
       "2706          347                  0\n",
       "2707         1618                  0\n",
       "2708         1207                  0\n",
       "2709           72                  0\n",
       "2710          201                  0\n",
       "2886         2049                  0\n",
       "2887           70                  0\n",
       "2888          170                  0\n",
       "2889          345                  0\n",
       "2890          227                  0\n",
       "2891          422                  0\n",
       "2892          740                  0\n",
       "2893          870                  0\n",
       "2894          777                  0\n",
       "2895          559                  0\n",
       "2896          619                  0\n",
       "2897          578                  0\n",
       "2898          670                  0\n",
       "2899          655                  0\n",
       "2900         1178                  0\n",
       "2901         1343                  0\n",
       "2902          652                  0\n",
       "2903          228                  0\n",
       "2904          471                  0\n",
       "2905          534                  0\n",
       "2906          605                  0\n",
       "2907          139                  0\n",
       "\n",
       "[784 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save for CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train.to_csv(\"X_train.csv\", index = False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Model selection based on which models do best in CV using default settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.65246553,  0.34753447])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.2648185, 'popularity_review'),\n",
       " (0.08350329, 'text_length'),\n",
       " (0.08188304, 'reviewsPerDay_user'),\n",
       " (0.08105388, 'reviewCount_user'),\n",
       " (0.0786505, 'deviatonReviewHotel'),\n",
       " (0.07042613, 'count_pronouns'),\n",
       " (0.0693068, 'popularity_user'),\n",
       " (0.06769878, 'friendCount_user'),\n",
       " (0.06392067, 'word_count'),\n",
       " (0.051405, 'count_targets'),\n",
       " (0.03442772, 'count_emotion_words'),\n",
       " (0.01676487, 'count_associated_actions'),\n",
       " (0.01464884, 'reviewSimilarity_user'),\n",
       " (0.01141371, 'filReviewCount_hotel'),\n",
       " (0.01007828, 'percentageReviews_user')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_v = [ 0.06769878,  0.08105388,  0.08188304,  0.01007828,  0.01464884,\n",
    "        0.01141371,  0.08350329,  0.06392067,  0.07042613,  0.01676487,\n",
    "        0.051405  ,  0.03442772,  0.0786505 ,  0.2648185 ,  0.0693068 ]\n",
    "\n",
    "fe_n = ['friendCount_user', 'reviewCount_user', 'reviewsPerDay_user',\n",
    "       'percentageReviews_user', 'reviewSimilarity_user',\n",
    "       'filReviewCount_hotel', 'text_length', 'word_count', 'count_pronouns',\n",
    "       'count_associated_actions', 'count_targets', 'count_emotion_words',\n",
    "       'deviatonReviewHotel', 'popularity_review', 'popularity_user']\n",
    "\n",
    "features = []\n",
    "for i in range(0, len(fe_v)):\n",
    "    features.append((fe_v[i], fe_n[i]))\n",
    "    \n",
    "features.sort(key = lambda x: x[0], reverse = True)\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.785020 (0.029273)\n",
      "LDA: 0.753718 (0.044304)\n",
      "KNN: 0.579314 (0.050661)\n",
      "CART: 0.741412 (0.045605)\n",
      "NB: 0.784471 (0.029872)\n",
      "NN: 0.650429 (0.127989)\n",
      "RF: 0.775056 (0.044334)\n",
      "AB: 0.770585 (0.049188)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABCAAAAHXCAYAAABtQytjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZlBjPvOrmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x840f684860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#inspired in http://machinelearningmastery.com/compare-machine-learning-algorithms-python-scikit-learn/\n",
    "\n",
    "import sklearn.model_selection as mds\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "#import xgboost as xgb\n",
    "\n",
    "def modelSelection():\n",
    "\n",
    "    # prepare data\n",
    "\n",
    "    #Y_train = train_df[:,-1]\n",
    "    #X_train = train_df[:,:-1]\n",
    "\n",
    "    #Y_train = train_df[len(train_df.columns)-1]\n",
    "    #X_train = train_df.drop(len(train_df.columns)-1,axis=1)\n",
    "\n",
    "    # prepare configuration for cross validation test harness\n",
    "    num_folds = 10\n",
    "    num_instances = len(X_train)\n",
    "\n",
    "    # prepare models\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('KNN', KNeighborsClassifier()))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    #models.append(('SVM-Linear', SVC(kernel=\"linear\")))\n",
    "    #models.append(('SVM-Poly', SVC(kernel=\"poly\")))\n",
    "    #models.append(('SVM-RBF', SVC(kernel=\"rbf\")))\n",
    "    models.append(('NN', MLPClassifier())) \n",
    "    models.append(('RF', RandomForestClassifier()))\n",
    "    models.append(('AB', AdaBoostClassifier()))\n",
    "    #models.append(('XGB', xgb.XGBClassifier()))\n",
    "\n",
    "    # evaluate each model in turn\n",
    "    results = []\n",
    "    scoring = 'roc_auc' # try with 'roc_auc', f1'\n",
    "\n",
    "    kfold = mds.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
    "\n",
    "    for model_name, model in models:\n",
    "        cv_results = mds.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "        results.append({\"name\": model_name, \"cv_results\": cv_results, \"mean\": cv_results.mean(), \"std\": cv_results.std()})\n",
    "        print(\"%s: %f (%f)\" % (model_name, cv_results.mean(), cv_results.std()))\n",
    "\n",
    "    # boxplot algorithm comparison\n",
    "    fig = plt.figure(figsize=(13, 5), dpi=500)\n",
    "    fig.suptitle('Algorithm Comparison')\n",
    "    ax = fig.add_subplot(111)\n",
    "    plt.boxplot([x[\"cv_results\"] for x in results])\n",
    "    ax.set_xticklabels([x[\"name\"] for x in results])\n",
    "    plt.show()\n",
    "\n",
    "    # order the models by the mean auc\n",
    "    #results_by_strategy.sort(key=lambda x: x[\"mean\"], reverse=True)\n",
    "    #print([(x[\"name\"], x[\"mean\"]) for x in results])\n",
    "\n",
    "modelSelection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results sorted by AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-ab1215b74a45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"mean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mna_method\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malgorithm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mna_method\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults_sorted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults_sorted\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results_sorted = [(na_method, algorithm[\"name\"], algorithm[\"mean\"]) for na_method in results for algorithm in results[na_method]]\n",
    "results_sorted.sort(key=lambda x: x[2], reverse=True)\n",
    "results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSupervisedAlgorithmsDefault(inf, sup):\n",
    "    plt.figure(figsize=(13, 7), dpi=500)\n",
    "    \n",
    "    # x axis\n",
    "    labels = [na_method for na_method in results]\n",
    "    labels.sort()\n",
    "    plt.xticks(np.arange(len(labels)), labels, rotation='vertical')\n",
    "    plt.ylim(inf, sup)\n",
    "    \n",
    "    # legend:\n",
    "    algorithm_names = [x[\"name\"] for x in results[\"01-zero\"]] \n",
    "    \n",
    "    [plt.plot([[x[\"mean\"] for x in results[na_method] if x[\"name\"] == alg_name] for na_method in sorted(results)],\n",
    "              label = alg_name) for alg_name in algorithm_names]\n",
    "    \n",
    "    plt.ylabel('AUC')\n",
    "    plt.xlabel('NA-filling method')\n",
    "    \n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.show()\n",
    "    \n",
    "plotSupervisedAlgorithmsDefault(0.69, 1)\n",
    "plotSupervisedAlgorithmsDefault(0.9, 1)\n",
    "plotSupervisedAlgorithmsDefault(0.99, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on this plot, we decided to tune XGB and LDA and use 07-spec-mean and 09-spec-min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotConfusionMatrixBestXGB():\n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    train, test = kfold.split(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "\n",
    "    lol = xgb.XGBClassifier().fit(datasets[\"07-spec-mean\"][\"train\"].iloc[train[0]], Y_train[train[0]])\n",
    "\n",
    "    train1_pred = lol.predict(datasets[\"07-spec-mean\"][\"train\"].iloc[train[1]])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[train[1]], train1_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    \n",
    "plotConfusionMatrixBestXGB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotConfusionMatrixBestLDA():\n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    train, test = kfold.split(datasets[\"09-spec-min\"][\"train\"], Y_train)\n",
    "\n",
    "    lol = xgb.XGBClassifier().fit(datasets[\"09-spec-min\"][\"train\"].iloc[train[0]], Y_train[train[0]])\n",
    "\n",
    "    train1_pred = lol.predict(datasets[\"09-spec-min\"][\"train\"].iloc[train[1]])\n",
    "\n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[train[1]], train1_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    \n",
    "plotConfusionMatrixBestLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 - Tuning of the best models\n",
    "#### Based on this plot, we decided to tune LDA and XGB\n",
    "### Tuning XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adapted from https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "def modelfit(alg, train_predictors, train_target, useTrainCV=True, cv_folds=10, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_predictors.values, label=train_target.values)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n",
    "            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=True)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    alg.fit(train_predictors, train_target, eval_metric='auc')\n",
    "        \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(train_predictors)\n",
    "    dtrain_predprob = alg.predict_proba(train_predictors)[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"Accuracy : %.4g\" % metrics.accuracy_score(train_target.values, dtrain_predictions))\n",
    "    print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(train_target, dtrain_predprob))\n",
    "                    \n",
    "    #feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    #feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    #plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics   #Additional scklearn functions\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "def tuneXGB1():\n",
    "    xgb1 = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5, # This should be between 3-10\n",
    "        min_child_weight=1, # A smaller value is chosen because it is a highly imbalanced class problem\n",
    "        gamma=0,\n",
    "        subsample=0.8, # Typical values range between 0.5-0.9.\n",
    "        colsample_bytree=0.8, # Typical values range between 0.5-0.9.\n",
    "        objective= 'binary:logistic',\n",
    "        #nthread=4,\n",
    "        scale_pos_weight=1, # Because of high class imbalance\n",
    "        seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This last result seems too good to be truth?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB2():\n",
    "    param_test1 = {\n",
    "        'max_depth': np.arange(3,10,2),\n",
    "        'min_child_weight': np.arange(1,6,2)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=5,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### {'max_depth': 7, 'min_child_weight': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB3():\n",
    "    param_test1 = {\n",
    "        'max_depth': [6,7,8],\n",
    "        'min_child_weight': [1,2,3]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=5,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB4():\n",
    "    param_test1 = {\n",
    "        'gamma':[i/10.0 for i in np.arange(0,5)]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gamma = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB5():\n",
    "    xgb1 = xgb.XGBClassifier(\n",
    "        learning_rate=0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=7, # This should be between 3-10\n",
    "        min_child_weight=1, # A smaller value is chosen because it is a highly imbalanced class problem\n",
    "        gamma=0,\n",
    "        subsample=0.8, # Typical values range between 0.5-0.9.\n",
    "        colsample_bytree=0.8, # Typical values range between 0.5-0.9.\n",
    "        objective= 'binary:logistic',\n",
    "        #nthread=4,\n",
    "        scale_pos_weight=1, # Because of high class imbalance\n",
    "        seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB6():\n",
    "    param_test1 = {\n",
    "     'subsample': np.arange(0.6, 1.0, 0.1),\n",
    "     'colsample_bytree': np.arange(0.6, 1.0, 0.1)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB6()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subsample = 0.6 and colsample_bytree = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB7():\n",
    "    param_test1 = {\n",
    "     'subsample': np.arange(0.55, 0.7, 0.05),\n",
    "     'colsample_bytree': np.arange(0.85, 1.0, 0.05)\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB7()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### better tuned: subsample=0.55 and colsample_bytree=0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneXGB8():\n",
    "    param_test1 = {\n",
    "     'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]\n",
    "    }\n",
    "    \n",
    "    gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=156, max_depth=7,\n",
    "                                                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "                                                      objective= 'binary:logistic', scale_pos_weight=1, seed=2), \n",
    "                            param_grid = param_test1, scoring='roc_auc',iid=False, cv=10)\n",
    "    \n",
    "    gsearch1.fit(datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    print(gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "    \n",
    "tuneXGB8()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reg alpha = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tuneXGB9():    \n",
    "    xgb1 = xgb.XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=7,\n",
    "                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "                      reg_alpha=1e-5, objective= 'binary:logistic', scale_pos_weight=1, seed=2)\n",
    "    \n",
    "    modelfit(xgb1, datasets[\"07-spec-mean\"][\"train\"], Y_train)\n",
    "    \n",
    "tuneXGB9()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuneLDA():\n",
    "    param_test1 = [{\"solver\": [\"svd\"], \"n_components\": np.arange(1,len(X_train.columns) - 1)},\n",
    "                   {\"solver\": [\"lsqr\", \"eigen\"], \"n_components\": np.arange(1,len(X_train.columns) - 1), \"shrinkage\": [\"auto\"]}]\n",
    "        \n",
    "    gsearch1 = GridSearchCV(estimator=LinearDiscriminantAnalysis(), \n",
    "                            param_grid = param_test1, scoring='roc_auc', cv=10)\n",
    "    \n",
    "    fit = gsearch1.fit(datasets[\"09-spec-min\"][\"train\"], Y_train)\n",
    "    return(fit)\n",
    "    \n",
    "bestLDAfit = tuneLDA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestLDAfit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bestLDAfit.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluatingBestLDA():\n",
    "    kfold = mds.StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=2)\n",
    "    model = LinearDiscriminantAnalysis(n_components = 1, shrinkage = \"auto\", solver=\"lsqr\")\n",
    "    \n",
    "    cv_results = mds.cross_val_score(model, datasets[\"09-spec-min\"][\"train\"], Y_train, cv=kfold, scoring=\"roc_auc\")\n",
    "    print(cv_results.mean())\n",
    "    \n",
    "evaluatingBestLDA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - Unsupervised Anomaly Detection Methods\n",
    "We decided to try LOF and see how it goes. We used our implementation from HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lof_pal as lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makePredictonsLOF():\n",
    "    outliers = []\n",
    "    \n",
    "    kfold = mds.StratifiedKFold(n_splits=2, shuffle=True, random_state=2)\n",
    "    sets,_ = kfold.split(datasets[\"07-spec-mean\"][\"train\"], Y_train)                                      \n",
    "    \n",
    "    # Train with only positive examples:\n",
    "    l = lof.LOF(datasets[\"07-spec-mean\"][\"train\"].iloc[sets[0]][Y_train[sets[0]] != 1], 3)\n",
    "    \n",
    "    Y_pred = [1 if x > 1.2 else 0 for x in l.predict(datasets[\"07-spec-mean\"][\"train\"].iloc[sets[1]])]\n",
    "        \n",
    "    plot_confusion_matrix(confusion_matrix(Y_train[sets[1]], Y_pred, labels = [0, 1]), classes = [0, 1])\n",
    "    #return Y_pred\n",
    "        \n",
    "    \n",
    "makePredictonsLOF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Submission to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose algorithm\n",
    "def makeSubmissionKaggle(algorithm):\n",
    "    algorithm.fit(X_train, Y_train)\n",
    "    Y_pred = algorithm.predict(X_test)\n",
    "    Y_pred = Y_pred.astype(int)\n",
    "\n",
    "    # save data to CSV\n",
    "    saveDataToCSV(Y_pred)\n",
    "    \n",
    "#makeSubmissionKaggle(\"07-spec-mean\", xgb.XGBClassifier( learning_rate=0.01, n_estimators=5000, max_depth=7,\n",
    "#                      min_child_weight=1, gamma=0, subsample=0.55, colsample_bytree=0.85,\n",
    "#                      reg_alpha=1e-5, objective= 'binary:logistic', scale_pos_weight=1, seed=2))\n",
    "\n",
    "#makeSubmissionKaggle(\"09-spec-min\", LinearDiscriminantAnalysis(n_components = 1, shrinkage = \"auto\", solver=\"lsqr\"))\n",
    "makeSubmissionKaggle(RandomForestClassifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
